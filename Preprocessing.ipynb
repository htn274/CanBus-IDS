{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4862dc0-9732-4a4b-bc24-81185d3ed8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import vaex\n",
    "import numpy as np\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eb7a90-50e4-4c49-a3cc-f946aa13504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_flag(sample):\n",
    "    if not isinstance(sample['Flag'], str):\n",
    "        col = 'Data' + str(sample['DLC'])\n",
    "        sample['Flag'] = sample[col]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c5069d-5c41-4f90-915c-2229bb0605f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_canid_bits(cid):\n",
    "    try:\n",
    "        s = bin(int(str(cid), 16))[2:].zfill(29)\n",
    "        bits = list(map(int, list(s)))\n",
    "        return bits\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a1f406-b7b9-40ce-b4d5-c45a28c72d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read by dask first\n",
    "attributes = ['Timestamp', 'canID', 'DLC', \n",
    "                           'Data0', 'Data1', 'Data2', \n",
    "                           'Data3', 'Data4', 'Data5', \n",
    "                           'Data6', 'Data7', 'Flag']\n",
    "folder = './Data/Car-Hacking/'\n",
    "attack_types = ['DoS', 'Fuzzy', 'gear', 'RPM']\n",
    "# attack = attack_types[1]\n",
    "# file_name = '{}{}_dataset.csv'.format(folder, attack)\n",
    "# print(file_name)\n",
    "# df = dd.read_csv(file_name, header=None, names=attributes)\n",
    "# for f in files[1]:\n",
    "#     print('Reading file: ', f)\n",
    "#     df = df.append(pd.read_csv(f, header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a0ac6d-4b12-4c56-8e7d-9a3241dec98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_name):\n",
    "    df = dd.read_csv(file_name, header=None, names=attributes)\n",
    "    print('Reading from {}: DONE'.format(file_name))\n",
    "    print('Dask processing: -------------')\n",
    "    df = df.apply(fill_flag, axis=1)\n",
    "    pd_df = df.compute()\n",
    "    pd_df = pd_df[['Timestamp', 'canID', 'Flag']]\n",
    "    pd_df['canBits'] = pd_df.canID.apply(convert_canid_bits)\n",
    "    print('Dask processing: DONE')\n",
    "    print('Aggregate data -----------------')\n",
    "    as_strided = np.lib.stride_tricks.as_strided  \n",
    "    test_df = pd_df.reset_index()\n",
    "    win = 29\n",
    "    v = as_strided(test_df.canBits, (len(test_df) - (win - 1), win), (test_df.canBits.values.strides * 2))\n",
    "    test_df['Flag'] = test_df['Flag'].apply(lambda x: True if x == 'T' else False)\n",
    "    test_df['features'] = pd.Series(v.tolist(), index=test_df.index[win - 1:])\n",
    "    # test_df['features'] = test_df.features.apply(lambda x: np.array(x).ravel().tolist())\n",
    "    v = as_strided(test_df.Flag, (len(test_df) - (win - 1), win), (test_df.Flag.values.strides * 2))\n",
    "    test_df['label'] = pd.Series(v.tolist(), index=test_df.index[win - 1:])\n",
    "    test_df = test_df.iloc[win - 1:]\n",
    "    test_df['label'] = test_df['label'].apply(lambda x: 1 if any(x) else 0)\n",
    "    print('Preprocessing: DONE')\n",
    "    return test_df[['features', 'label']].reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bed741d-6ad9-4ef5-9512-3983b4e313d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(df):\n",
    "    print('Create train - test - val: ')\n",
    "    train, test = train_test_split(df, test_size=0.3, shuffle=True)\n",
    "    train, val = train_test_split(train, test_size=0.2, shuffle=True)\n",
    "    train_ul, train_l = train_test_split(train, test_size=0.1, shuffle=True)\n",
    "    train_ul = train_ul.reset_index().drop(['index'], axis=1)\n",
    "    train_l = train_l.reset_index().drop(['index'], axis=1)\n",
    "    test = test.reset_index().drop(['index'], axis=1)\n",
    "    val = val.reset_index().drop(['index'], axis=1)\n",
    "    \n",
    "    data_info = {\n",
    "        \"train_unlabel\": train_ul.shape[0],\n",
    "        \"train_label\": train_l.shape[0],\n",
    "        \"validation\": val.shape[0],\n",
    "        \"test\": test.shape[0]\n",
    "    }\n",
    "    \n",
    "    return data_info, train_ul, train_l, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa34d96-9a47-4bc5-a210-6cf714cf9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(x, y):\n",
    "    \"\"\"converts x, y to tf.train.Example and serialize\"\"\"\n",
    "    #Need to pay attention to whether it needs to be converted to numpy() form\n",
    "    input_features = tf.train.Int64List(value = np.array(x).flatten())\n",
    "    label = tf.train.Int64List(value = np.array([y]))\n",
    "    features = tf.train.Features(\n",
    "        feature = {\n",
    "            \"input_features\": tf.train.Feature(int64_list = input_features),\n",
    "            \"label\" : tf.train.Feature(int64_list = label)\n",
    "        }\n",
    "    )\n",
    "    example = tf.train.Example(features = features)\n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3279e1e7-038a-48c7-a643-ef50b9b93b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord(data, filename):\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(filename)\n",
    "    for _, row in tqdm(data.iterrows()):\n",
    "        tfrecord_writer.write(serialize_example(row['features'], row['label']))\n",
    "    tfrecord_writer.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e5ecbc-5621-4e05-8cb9-49a1fd9b8315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 9 µs, total: 9 µs\n",
      "Wall time: 16.7 µs\n",
      "./Data/Car-Hacking/Fuzzy_dataset.csv---------------------------\n",
      "Reading from ./Data/Car-Hacking/Fuzzy_dataset.csv: DONE\n",
      "Dask processing: -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/dask/dataframe/core.py:4503: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'object', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'object', 'Data7': 'object', 'Flag': 'object'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "Create train - test - val: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  ./Data/Fuzzy/\n",
      "Writing train_unlabel.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1934770it [10:54, 2954.10it/s]\n",
      "570it [00:00, 2896.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_label.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "214975it [01:11, 2997.53it/s]\n",
      "64it [00:00, 638.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1151650it [06:20, 3026.12it/s]\n",
      "184it [00:00, 1837.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing val.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "537437it [03:06, 2876.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data info\n",
      "==========================================\n",
      "./Data/Car-Hacking/gear_dataset.csv---------------------------\n",
      "Reading from ./Data/Car-Hacking/gear_dataset.csv: DONE\n",
      "Dask processing: -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/dask/dataframe/core.py:4503: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'object', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'object', 'Data7': 'object', 'Flag': 'object'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "Create train - test - val: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  ./Data/gear/\n",
      "Writing train_unlabel.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2239328it [12:46, 2921.27it/s]\n",
      "238it [00:00, 2379.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_label.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "248815it [01:29, 2769.91it/s]\n",
      "315it [00:00, 1836.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1332935it [07:37, 2912.73it/s]\n",
      "145it [00:00, 1448.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing val.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "622036it [03:25, 3025.76it/s]\n",
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/dask/dataframe/core.py:4503: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'object', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'object', 'Data7': 'object', 'Flag': 'object'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data info\n",
      "==========================================\n",
      "./Data/Car-Hacking/RPM_dataset.csv---------------------------\n",
      "Reading from ./Data/Car-Hacking/RPM_dataset.csv: DONE\n",
      "Dask processing: -------------\n",
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "Create train - test - val: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  ./Data/RPM/\n",
      "Writing train_unlabel.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2329322it [12:51, 3019.52it/s]\n",
      "252it [00:00, 2514.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_label.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "258814it [01:23, 3106.57it/s]\n",
      "1it [00:00,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386503it [07:30, 3074.51it/s]\n",
      "463it [00:00, 2450.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing val.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "647035it [03:27, 3111.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data info\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for attack in attack_types[1:]:\n",
    "    file_name = '{}{}_dataset.csv'.format(folder, attack)\n",
    "    print(file_name + '---------------------------')\n",
    "    df = preprocess(file_name)\n",
    "    data_info, train_ul, train_l, val, test = create_train_test(df)\n",
    "    save_path = './Data/{}/'.format(attack)\n",
    "    print('Path: ', save_path)\n",
    "    print('Writing train_unlabel.......................')\n",
    "    write_tfrecord(train_ul, save_path + \"train_unlabel\")\n",
    "    print('Writing train_label.......................')\n",
    "    write_tfrecord(train_l, save_path + \"train_label\")\n",
    "    print('Writing test.......................')\n",
    "    write_tfrecord(test, save_path + \"test\")\n",
    "    print('Writing val.......................')\n",
    "    write_tfrecord(val, save_path + \"val\")\n",
    "    print('Writing data info')\n",
    "    json.dump(data_info, open(save_path + 'datainfo.txt', 'w'))\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4246e0f8-fdde-47bc-b810-a09f367c7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_path + 'datainfo.txt', 'w') as f:\n",
    "#     f.write('Train Unlabel: {}\\n'.format(train_ul.shape[0]))\n",
    "#     f.write('Train Label: {}\\n'.format(train_l.shape[0]))\n",
    "#     f.write('Test: {}\\n'.format(test.shape[0]))\n",
    "#     f.write('Validation: {}\\n'.format(val.shape[0]))\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ec23c-cc04-41c8-b14f-7a9dc3fac8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data = tf.data.TFRecordDataset('data')\n",
    "# feature_description = {\n",
    "#     'input_features': tf.io.FixedLenFeature([29*29], tf.int64),\n",
    "#     'label': tf.io.FixedLenFeature([1], tf.int64)\n",
    "# }\n",
    "\n",
    "# def _parse_image_function(example_proto):\n",
    "#   # Parse the input tf.train.Example proto using the dictionary above.\n",
    "#   return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "# parsed_image_dataset = raw_data.map(_parse_image_function)\n",
    "# parsed_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ded4251-59c7-4a44-9fe6-c8e747c32cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_features in parsed_image_dataset:\n",
    "#     image_raw = image_features['input_features'].numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow1.x] *",
   "language": "python",
   "name": "conda-env-Tensorflow1.x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
