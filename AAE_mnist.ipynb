{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd226837-2a21-4306-bda3-139559741f56",
   "metadata": {},
   "source": [
    "# Tutorial from [here](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-4-classify-mnist-using-1000-labels-2ca08071f95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a874933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39968faa-8382-4403-828b-b167ef10f335",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f44c62a8-6635-4a95-9cf2-788f8c68c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_dim = 28*28 #29 * 29\n",
    "n_l1 = 1000\n",
    "n_l2 = 1000\n",
    "z_dim = 10\n",
    "batch_size = 100\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "results_path = './Results/'\n",
    "n_labels = 10\n",
    "n_labeled = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038fd9e1-c55d-41f1-85dd-12f94f9e1153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dense(x, n1, n2, name):\n",
    "    \"\"\"\n",
    "    Used to create a dense layer.\n",
    "    :param x: input tensor to the dense layer\n",
    "    :param n1: no. of input neurons\n",
    "    :param n2: no. of output neurons\n",
    "    :param name: name of the entire dense layer.i.e, variable scope name.\n",
    "    :return: tensor with shape [batch_size, n2]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name, reuse=None):\n",
    "        weights = tf.get_variable(\"weights\", shape=[n1, n2],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0., stddev=0.01))\n",
    "        bias = tf.get_variable(\"bias\", shape=[n2], initializer=tf.constant_initializer(0.0))\n",
    "        out = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da92ad2-b973-4cbc-b1b0-f43b0df5c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, reuse=False, supervised=False):\n",
    "    \"\"\"\n",
    "    Encode part of the autoencoder.\n",
    "    :param x: input to the autoencoder\n",
    "    :param reuse: True -> Reuse the encoder variables, False -> Create or search of variables before creating\n",
    "    :param supervised: True -> returns output without passing it through softmax,\n",
    "                       False -> returns output after passing it through softmax.\n",
    "    :return: tensor which is the classification output and a hidden latent variable of the autoencoder.\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Encoder'):\n",
    "        e_dense_1 = tf.nn.relu(dense(x, input_dim, n_l1, 'e_dense_1'))\n",
    "        e_dense_2 = tf.nn.relu(dense(e_dense_1, n_l1, n_l2, 'e_dense_2'))\n",
    "        latent_variable = dense(e_dense_2, n_l2, z_dim, 'e_latent_variable')\n",
    "        cat_op = dense(e_dense_2, n_l2, n_labels, 'e_label')\n",
    "        if not supervised:\n",
    "            softmax_label = tf.nn.softmax(logits=cat_op, name='e_softmax_label')\n",
    "        else:\n",
    "            softmax_label = cat_op\n",
    "        return softmax_label, latent_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4472adb1-eb24-4062-bb81-f2c00e30fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x, reuse=False):\n",
    "    \"\"\"\n",
    "    Decoder part of the autoencoder.\n",
    "    :param x: input to the decoder\n",
    "    :param reuse: True -> Reuse the decoder variables, False -> Create or search of variables before creating\n",
    "    :return: tensor which should ideally be the input given to the encoder.\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Decoder'):\n",
    "        d_dense_1 = tf.nn.relu(dense(x, z_dim + n_labels, n_l2, 'd_dense_1'))\n",
    "        d_dense_2 = tf.nn.relu(dense(d_dense_1, n_l2, n_l1, 'd_dense_2'))\n",
    "        output = tf.nn.sigmoid(dense(d_dense_2, n_l1, input_dim, 'd_output'))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "955af04d-3c32-4e9c-8e21-e36119dbfbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_gauss(x, reuse=False):\n",
    "    \"\"\"\n",
    "    Discriminator that is used to match the posterior distribution with a given gaussian distribution.\n",
    "    :param x: tensor of shape [batch_size, z_dim]\n",
    "    :param reuse: True -> Reuse the discriminator variables,\n",
    "                  False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Discriminator_Gauss'):\n",
    "        dc_den1 = tf.nn.relu(dense(x, z_dim, n_l1, name='dc_g_den1'))\n",
    "        dc_den2 = tf.nn.relu(dense(dc_den1, n_l1, n_l2, name='dc_g_den2'))\n",
    "        output = dense(dc_den2, n_l2, 1, name='dc_g_output')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e096fb7-1e2f-4102-ba4f-8927b10f182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_categorical(x, reuse=False):\n",
    "    \"\"\"\n",
    "    Discriminator that is used to match the posterior distribution with a given categorical distribution.\n",
    "    :param x: tensor of shape [batch_size, n_labels]\n",
    "    :param reuse: True -> Reuse the discriminator variables,\n",
    "                  False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Discriminator_Categorial'):\n",
    "        dc_den1 = tf.nn.relu(dense(x, n_labels, n_l1, name='dc_c_den1'))\n",
    "        dc_den2 = tf.nn.relu(dense(dc_den1, n_l1, n_l2, name='dc_c_den2'))\n",
    "        output = dense(dc_den2, n_l2, 1, name='dc_c_output')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a4ae2df-d18a-4729-89d9-999ede99e423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-ebf80e480869>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./Data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./Data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./Data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Get the MNIST data\n",
    "mnist = input_data.read_data_sets('./Data', one_hot=True)\n",
    "\n",
    "# Placeholders for input data and the targets\n",
    "x_input = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Input')\n",
    "x_input_l = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Labeled_Input')\n",
    "y_input = tf.placeholder(dtype=tf.float32, shape=[batch_size, n_labels], name='Labels')\n",
    "x_target = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Target')\n",
    "real_distribution = tf.placeholder(dtype=tf.float32, shape=[batch_size, z_dim], name='Real_distribution')\n",
    "categorial_distribution = tf.placeholder(dtype=tf.float32, shape=[batch_size, n_labels],\n",
    "                                         name='Categorical_distribution')\n",
    "manual_decoder_input = tf.placeholder(dtype=tf.float32, shape=[1, z_dim + n_labels], name='Decoder_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091e17f5-417f-42b4-bacd-5685f241d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction Phase\n",
    "# Encoder try to predict both label and latent space of the input, which will be feed into Decoder to reconstruct the input\n",
    "# The process is optimized by autoencoder_loss which is the MSE of the decoder_output and the orginal input\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    encoder_output_label, encoder_output_latent = encoder(x_input)\n",
    "    decoder_input = tf.concat([encoder_output_label, encoder_output_latent], 1)\n",
    "    decoder_output = decoder(decoder_input)\n",
    "\n",
    "autoencoder_loss = tf.reduce_mean(tf.square(x_target - decoder_output))\n",
    "autoencoder_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(autoencoder_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24de2af8-2386-423a-b6e5-951bc865a465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Regularization Phase\n",
    "# Train both 2 discriminator of gaussian and categorical to detect the output from encoder\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    # Discriminator for gaussian\n",
    "    d_g_real = discriminator_gauss(real_distribution)\n",
    "    d_g_fake = discriminator_gauss(encoder_output_latent, reuse=True)\n",
    "# Need to seperate dicriminator of gaussian and categorical\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    # Discrimnator for categorical\n",
    "    d_c_real = discriminator_categorical(categorial_distribution)\n",
    "    d_c_fake = discriminator_categorical(encoder_output_label, reuse=True)\n",
    "\n",
    "# Discriminator gaussian loss \n",
    "dc_g_loss_real = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_g_real), logits=d_g_real))\n",
    "dc_g_loss_fake = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_g_fake), logits=d_g_fake))\n",
    "dc_g_loss = dc_g_loss_real + dc_g_loss_fake\n",
    "\n",
    "# Discriminator categorical loss\n",
    "dc_c_loss_real = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_c_real), logits=d_c_real))\n",
    "dc_c_loss_fake = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_c_fake), logits=d_c_fake))\n",
    "dc_c_loss = dc_c_loss_fake + dc_c_loss_real\n",
    "\n",
    "all_variables = tf.trainable_variables()\n",
    "dc_g_var = [var for var in all_variables if 'dc_g_' in var.name]\n",
    "dc_c_var = [var for var in all_variables if 'dc_c_' in var.name]\n",
    "discriminator_g_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                                       beta1=beta1).minimize(dc_g_loss, var_list=dc_g_var)\n",
    "discriminator_c_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                                       beta1=beta1).minimize(dc_c_loss, var_list=dc_c_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38b8d6ff-5725-4156-91da-82745a5a8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator loss\n",
    "generator_g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_g_fake), logits=d_g_fake))\n",
    "generator_c_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_c_fake), logits=d_c_fake))\n",
    "generator_loss = generator_g_loss + generator_c_loss\n",
    "\n",
    "en_var = [var for var in all_variables if 'e_' in var.name]\n",
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(generator_loss, var_list=en_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f6c02a-da3d-4b0f-ba3a-a89bb8bf3b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-3455a15bfac4>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Semi-Supervised Classification Phase\n",
    "# Train encoder with a small amount of label samples\n",
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    encoder_output_label_, _ = encoder(x_input_l, reuse=True, supervised=True)\n",
    "    \n",
    "# Classification accuracy of encoder\n",
    "correct_pred = tf.equal(tf.argmax(encoder_output_label_, 1), tf.argmax(y_input, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "supervised_encoder_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=encoder_output_label_))\n",
    "supervised_encoder_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(supervised_encoder_loss, var_list=en_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10dc6c5e-43f0-4219-8265-a23272fd0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_results():\n",
    "    \"\"\"\n",
    "    Forms folders for each run to store the tensorboard files, saved models and the log files.\n",
    "    :return: three string pointing to tensorboard, saved models and log paths respectively.\n",
    "    \"\"\"\n",
    "    folder_name = \"/{0}_{1}_{2}_{3}_{4}_{5}_Semi_Supervised\". \\\n",
    "        format(datetime.datetime.now(), z_dim, learning_rate, batch_size, n_epochs, beta1)\n",
    "    tensorboard_path = results_path + folder_name + '/Tensorboard'\n",
    "    saved_model_path = results_path + folder_name + '/Saved_models/'\n",
    "    log_path = results_path + folder_name + '/log'\n",
    "    if not os.path.exists(results_path + folder_name):\n",
    "        os.mkdir(results_path + folder_name)\n",
    "        os.mkdir(tensorboard_path)\n",
    "        os.mkdir(saved_model_path)\n",
    "        os.mkdir(log_path)\n",
    "    return tensorboard_path, saved_model_path, log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1d04248-4a96-4912-ab3f-617554dee71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(x, y, batch_size):\n",
    "    \"\"\"\n",
    "    Used to return a random batch from the given inputs.\n",
    "    :param x: Input images of shape [None, 784]\n",
    "    :param y: Input labels of shape [None, 10]\n",
    "    :param batch_size: integer, batch size of images and labels to return\n",
    "    :return: x -> [batch_size, 784], y-> [batch_size, 10]\n",
    "    \"\"\"\n",
    "    index = np.arange(n_labeled)\n",
    "    random_index = np.random.permutation(index)[:batch_size]\n",
    "    return x[random_index], y[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e5a591c-6cb6-497e-bcfa-19f8859f25cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Autoencoder Loss is illegal; using Autoencoder_Loss instead.\n",
      "INFO:tensorflow:Summary name Discriminator gauss Loss is illegal; using Discriminator_gauss_Loss instead.\n",
      "INFO:tensorflow:Summary name Discriminator categorical Loss is illegal; using Discriminator_categorical_Loss instead.\n",
      "INFO:tensorflow:Summary name Generator Loss is illegal; using Generator_Loss instead.\n",
      "INFO:tensorflow:Summary name Supervised Encoder Loss is illegal; using Supervised_Encoder_Loss instead.\n",
      "INFO:tensorflow:Summary name Encoder Gauss Distribution is illegal; using Encoder_Gauss_Distribution instead.\n",
      "INFO:tensorflow:Summary name Real Gauss Distribution is illegal; using Real_Gauss_Distribution instead.\n",
      "INFO:tensorflow:Summary name Encoder Categorical Distribution is illegal; using Encoder_Categorical_Distribution instead.\n",
      "INFO:tensorflow:Summary name Real Categorical Distribution is illegal; using Real_Categorical_Distribution instead.\n",
      "INFO:tensorflow:Summary name Input Images is illegal; using Input_Images instead.\n",
      "INFO:tensorflow:Summary name Generated Images is illegal; using Generated_Images instead.\n",
      "------------------Epoch 0/1000------------------\n",
      "Epoch: 0, iteration: 5\n",
      "Autoencoder Loss: 0.1873217523097992\n",
      "Discriminator Gauss Loss: 1.1589069366455078\n",
      "Discriminator Categorical Loss: 1.3349075317382812\n",
      "Generator Loss: 1.1477421522140503\n",
      "Supervised Loss: 2.4196577072143555\n",
      "\n",
      "Epoch: 0, iteration: 10\n",
      "Autoencoder Loss: 0.08647259324789047\n",
      "Discriminator Gauss Loss: 0.7632445693016052\n",
      "Discriminator Categorical Loss: 1.1989648342132568\n",
      "Generator Loss: 1.3848090171813965\n",
      "Supervised Loss: 12.975327491760254\n",
      "\n",
      "Encoder Classification Accuracy: 0.1935999995470047\n",
      "------------------Epoch 1/1000------------------\n",
      "Epoch: 1, iteration: 5\n",
      "Autoencoder Loss: 0.08167681843042374\n",
      "Discriminator Gauss Loss: 0.7178546786308289\n",
      "Discriminator Categorical Loss: 1.1254327297210693\n",
      "Generator Loss: 1.3127827644348145\n",
      "Supervised Loss: 7.934621810913086\n",
      "\n",
      "Epoch: 1, iteration: 10\n",
      "Autoencoder Loss: 0.07464025169610977\n",
      "Discriminator Gauss Loss: 0.6864854097366333\n",
      "Discriminator Categorical Loss: 1.426421880722046\n",
      "Generator Loss: 1.1490538120269775\n",
      "Supervised Loss: 2.4962363243103027\n",
      "\n",
      "Encoder Classification Accuracy: 0.10019999999552966\n",
      "------------------Epoch 2/1000------------------\n",
      "Epoch: 2, iteration: 5\n",
      "Autoencoder Loss: 0.07326968759298325\n",
      "Discriminator Gauss Loss: 0.6735101938247681\n",
      "Discriminator Categorical Loss: 1.0643889904022217\n",
      "Generator Loss: 1.7259151935577393\n",
      "Supervised Loss: 2.577895402908325\n",
      "\n",
      "Epoch: 2, iteration: 10\n",
      "Autoencoder Loss: 0.0695575699210167\n",
      "Discriminator Gauss Loss: 0.66010582447052\n",
      "Discriminator Categorical Loss: 0.8598536252975464\n",
      "Generator Loss: 1.9415557384490967\n",
      "Supervised Loss: 2.748737096786499\n",
      "\n",
      "Encoder Classification Accuracy: 0.10159999988973141\n",
      "------------------Epoch 3/1000------------------\n",
      "Epoch: 3, iteration: 5\n",
      "Autoencoder Loss: 0.07592923939228058\n",
      "Discriminator Gauss Loss: 0.610987663269043\n",
      "Discriminator Categorical Loss: 0.8640822172164917\n",
      "Generator Loss: 1.6763638257980347\n",
      "Supervised Loss: 3.578165054321289\n",
      "\n",
      "Epoch: 3, iteration: 10\n",
      "Autoencoder Loss: 0.06889121234416962\n",
      "Discriminator Gauss Loss: 0.5205624103546143\n",
      "Discriminator Categorical Loss: 0.8485438823699951\n",
      "Generator Loss: 1.8815834522247314\n",
      "Supervised Loss: 4.894557952880859\n",
      "\n",
      "Encoder Classification Accuracy: 0.10700000010430813\n",
      "------------------Epoch 4/1000------------------\n",
      "Epoch: 4, iteration: 5\n",
      "Autoencoder Loss: 0.07355279475450516\n",
      "Discriminator Gauss Loss: 0.5469686985015869\n",
      "Discriminator Categorical Loss: 0.7828364372253418\n",
      "Generator Loss: 3.333346366882324\n",
      "Supervised Loss: 3.7737579345703125\n",
      "\n",
      "Epoch: 4, iteration: 10\n",
      "Autoencoder Loss: 0.07857362180948257\n",
      "Discriminator Gauss Loss: 0.655236542224884\n",
      "Discriminator Categorical Loss: 0.7023591995239258\n",
      "Generator Loss: 1.9480366706848145\n",
      "Supervised Loss: 2.3309600353240967\n",
      "\n",
      "Encoder Classification Accuracy: 0.1011999998241663\n",
      "------------------Epoch 5/1000------------------\n",
      "Epoch: 5, iteration: 5\n",
      "Autoencoder Loss: 0.0732639953494072\n",
      "Discriminator Gauss Loss: 0.5909450650215149\n",
      "Discriminator Categorical Loss: 0.5590585470199585\n",
      "Generator Loss: 2.753967761993408\n",
      "Supervised Loss: 2.427100896835327\n",
      "\n",
      "Epoch: 5, iteration: 10\n",
      "Autoencoder Loss: 0.06744612753391266\n",
      "Discriminator Gauss Loss: 0.6042108535766602\n",
      "Discriminator Categorical Loss: 0.5721889138221741\n",
      "Generator Loss: 3.051176071166992\n",
      "Supervised Loss: 2.371983766555786\n",
      "\n",
      "Encoder Classification Accuracy: 0.09579999975860119\n",
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "------------------Epoch 6/1000------------------\n",
      "Epoch: 6, iteration: 5\n",
      "Autoencoder Loss: 0.06757127493619919\n",
      "Discriminator Gauss Loss: 0.40895235538482666\n",
      "Discriminator Categorical Loss: 0.37657514214515686\n",
      "Generator Loss: 3.9251790046691895\n",
      "Supervised Loss: 2.3060872554779053\n",
      "\n",
      "Epoch: 6, iteration: 10\n",
      "Autoencoder Loss: 0.0679214671254158\n",
      "Discriminator Gauss Loss: 0.311111718416214\n",
      "Discriminator Categorical Loss: 0.39507052302360535\n",
      "Generator Loss: 3.587773084640503\n",
      "Supervised Loss: 2.2498745918273926\n",
      "\n",
      "Encoder Classification Accuracy: 0.20099999994039536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-db24670a0432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/log.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encoder Classification Accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaved_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Get the latest results folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1201\u001b[0m               \u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m               \u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m               save_debug_info=save_debug_info)\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         save_debug_info=save_debug_info)\n\u001b[0m\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs, save_debug_info, **kwargs)\u001b[0m\n\u001b[1;32m   1587\u001b[0m       \u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m       \u001b[0msave_debug_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_debug_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1590\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, saver_def, clear_extraneous_savers, strip_default_attrs, save_debug_info, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m       \u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m       \u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mcreate_meta_graph_def\u001b[0;34m(meta_info_def, graph_def, saver_def, collection_list, graph, export_scope, exclude_nodes, clear_extraneous_savers, strip_default_attrs)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m   \u001b[0;31m# Fills in meta_info_def.stripped_op_list using the ops from graph_def.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Reshape immages to display them\n",
    "input_images = tf.reshape(x_input, [-1, 28, 28, 1])\n",
    "generated_images = tf.reshape(decoder_output, [-1, 28, 28, 1])\n",
    "\n",
    "# Tensorboard visualization\n",
    "tf.summary.scalar(name='Autoencoder Loss', tensor=autoencoder_loss)\n",
    "tf.summary.scalar(name='Discriminator gauss Loss', tensor=dc_g_loss)\n",
    "tf.summary.scalar(name='Discriminator categorical Loss', tensor=dc_c_loss)\n",
    "tf.summary.scalar(name='Generator Loss', tensor=generator_loss)\n",
    "tf.summary.scalar(name='Supervised Encoder Loss', tensor=supervised_encoder_loss)\n",
    "# tf.summary.scalar(name='Supervised Encoder Accuracy', tensor=accuracy)\n",
    "tf.summary.histogram(name='Encoder Gauss Distribution', values=encoder_output_latent)\n",
    "tf.summary.histogram(name='Real Gauss Distribution', values=real_distribution)\n",
    "tf.summary.histogram(name='Encoder Categorical Distribution', values=encoder_output_label)\n",
    "tf.summary.histogram(name='Real Categorical Distribution', values=categorial_distribution)\n",
    "tf.summary.image(name='Input Images', tensor=input_images, max_outputs=10)\n",
    "tf.summary.image(name='Generated Images', tensor=generated_images, max_outputs=10)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "accuracies = []\n",
    "# Saving the model\n",
    "saver = tf.train.Saver()\n",
    "step = 0\n",
    "with tf.Session() as sess:\n",
    "    if True:\n",
    "        tensorboard_path, saved_model_path, log_path = form_results()\n",
    "        sess.run(init)\n",
    "        writer = tf.summary.FileWriter(logdir=tensorboard_path, graph=sess.graph)\n",
    "        x_l, y_l = mnist.test.next_batch(n_labeled)\n",
    "        for i in range(n_epochs):\n",
    "            n_batches = int(n_labeled / batch_size)\n",
    "            print(\"------------------Epoch {}/{}------------------\".format(i, n_epochs))\n",
    "            for b in range(1, n_batches + 1):\n",
    "                z_real_dist = np.random.randn(batch_size, z_dim) * 5.\n",
    "                real_cat_dist = np.random.randint(low=0, high=10, size=batch_size)\n",
    "                real_cat_dist = np.eye(n_labels)[real_cat_dist]\n",
    "                batch_x_ul, _ = mnist.train.next_batch(batch_size)\n",
    "                batch_x_l, batch_y_l = next_batch(x_l, y_l, batch_size=batch_size)\n",
    "                \n",
    "                sess.run(autoencoder_optimizer, feed_dict={x_input: batch_x_ul, x_target: batch_x_ul})\n",
    "                sess.run(discriminator_g_optimizer,\n",
    "                         feed_dict={x_input: batch_x_ul, x_target: batch_x_ul, real_distribution: z_real_dist})\n",
    "                sess.run(discriminator_c_optimizer,\n",
    "                         feed_dict={x_input: batch_x_ul, x_target: batch_x_ul,\n",
    "                                    categorial_distribution: real_cat_dist})\n",
    "                sess.run(generator_optimizer, feed_dict={x_input: batch_x_ul, x_target: batch_x_ul})\n",
    "                sess.run(supervised_encoder_optimizer, feed_dict={x_input_l: batch_x_l, y_input: batch_y_l})\n",
    "                if b % 5 == 0:\n",
    "                    a_loss, d_g_loss, d_c_loss, g_loss, s_loss, summary = sess.run(\n",
    "                        [autoencoder_loss, dc_g_loss, dc_c_loss, generator_loss, supervised_encoder_loss,\n",
    "                         summary_op],\n",
    "                        feed_dict={x_input: batch_x_ul, x_target: batch_x_ul,\n",
    "                                   real_distribution: z_real_dist, y_input: batch_y_l, x_input_l: batch_x_l,\n",
    "                                   categorial_distribution: real_cat_dist})\n",
    "                    writer.add_summary(summary, global_step=step)\n",
    "                    print(\"Epoch: {}, iteration: {}\".format(i, b))\n",
    "                    print(\"Autoencoder Loss: {}\".format(a_loss))\n",
    "                    print(\"Discriminator Gauss Loss: {}\".format(d_g_loss))\n",
    "                    print(\"Discriminator Categorical Loss: {}\".format(d_c_loss))\n",
    "                    print(\"Generator Loss: {}\".format(g_loss))\n",
    "                    print(\"Supervised Loss: {}\\n\".format(s_loss))\n",
    "                    with open(log_path + '/log.txt', 'a') as log:\n",
    "                        log.write(\"Epoch: {}, iteration: {}\\n\".format(i, b))\n",
    "                        log.write(\"Autoencoder Loss: {}\\n\".format(a_loss))\n",
    "                        log.write(\"Discriminator Gauss Loss: {}\".format(d_g_loss))\n",
    "                        log.write(\"Discriminator Categorical Loss: {}\".format(d_c_loss))\n",
    "                        log.write(\"Generator Loss: {}\\n\".format(g_loss))\n",
    "                        log.write(\"Supervised Loss: {}\".format(s_loss))\n",
    "                step += 1\n",
    "            # Calculate the accuracy on validation set\n",
    "            acc = 0\n",
    "            num_batches = int(mnist.validation.num_examples/batch_size)\n",
    "            for j in range(num_batches):\n",
    "                # Classify unseen validation data instead of test data or train data\n",
    "                batch_x_l, batch_y_l = mnist.validation.next_batch(batch_size=batch_size)\n",
    "                encoder_acc = sess.run(accuracy, feed_dict={x_input_l: batch_x_l, y_input: batch_y_l})\n",
    "                acc += encoder_acc\n",
    "            acc /= num_batches\n",
    "            accuracies.append(acc)\n",
    "            print(\"Encoder Classification Accuracy: {}\".format(acc))\n",
    "            with open(log_path + '/log.txt', 'a') as log:\n",
    "                log.write(\"Encoder Classification Accuracy: {}\".format(acc))\n",
    "            saver.save(sess, save_path=saved_model_path, global_step=step)\n",
    "    else:\n",
    "        # Get the latest results folder\n",
    "        all_results = os.listdir(results_path)\n",
    "        all_results.sort()\n",
    "        saver.restore(sess, save_path=tf.train.latest_checkpoint(results_path + '/' +\n",
    "                                                                 all_results[-1] + '/Saved_models/'))\n",
    "        generate_image_grid(sess, op=decoder_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae43fb-d12b-403c-a6e8-30edff4d6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64493a-c0c4-461d-8e3d-03dfe6958a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow1.x] *",
   "language": "python",
   "name": "conda-env-Tensorflow1.x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
