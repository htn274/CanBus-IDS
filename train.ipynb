{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f2229d-8c22-4e4d-b499-d3f42ca446a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28eb233e-5b63-4063-9cb7-c8c2297f1347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import encoder, decoder, discriminator_gauss, discriminator_categorical\n",
    "from utils import read_tfrecord, data_from_tfrecord, data_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db494cdf-d859-49b2-820f-4389299e270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data info:  {'train_unlabel': 10798551, 'train_label': 800000, 'validation': 2485403, 'test': 2485403}\n"
     ]
    }
   ],
   "source": [
    "# Run this when train with all data\n",
    "data_info = {\n",
    "   \"train_unlabel\": 0, \n",
    "    \"train_label\": 0, \n",
    "    \"validation\": 0, \n",
    "    \"test\": 0\n",
    "}\n",
    "labels = ['DoS', 'Fuzzy', 'gear', 'RPM', 'Normal']\n",
    "for f in ['./Data/{}/datainfo.txt'.format(l) for l in labels]:\n",
    "    data_read = json.load(open(f))\n",
    "    for key in data_info.keys():\n",
    "        data_info[key] += data_read[key]\n",
    "        \n",
    "attack = 'all' # DoS, Fuzzy, gear, RPM, all\n",
    "results_path = './Results/{}/'.format(attack)\n",
    "data_path = './Data/*/'\n",
    "\n",
    "print('Data info: ', data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbeaed5a-5687-41e3-9447-ed4730b917e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train unlabel paths:  ['./Data/DoS/train_unlabel', './Data/gear/train_unlabel', './Data/RPM/train_unlabel', './Data/Fuzzy/train_unlabel']\n",
      "Train label paths:  ['./Data/gear/train_label', './Data/RPM/train_label', './Data/Fuzzy/train_label']\n",
      "Validation paths:  ['./Data/gear/val', './Data/RPM/val', './Data/Fuzzy/val']\n",
      "Data info: {'train_unlabel': 6503420, 'train_label': 722604, 'validation': 1806508, 'test': 3871088}\n"
     ]
    }
   ],
   "source": [
    "# Run this when train with unknown attack\n",
    "data_info = {\n",
    "   \"train_unlabel\": 0, \n",
    "    \"train_label\": 0, \n",
    "    \"validation\": 0, \n",
    "    \"test\": 0\n",
    "}\n",
    "attacks = ['DoS', 'gear', 'RPM', 'Fuzzy']\n",
    "unknown_attack_idx = 0\n",
    "results_path = './Results/unknown_attack/{}'.format(attacks[unknown_attack_idx])\n",
    "# Put all into unlabel\n",
    "train_unlabel_paths = ['./Data/{}/train_unlabel'.format(x) for x in attacks]\n",
    "train_label_paths = ['./Data/{}/train_label'.format(x) for x in attacks if x is not attacks[unknown_attack_idx]]\n",
    "val_paths = ['./Data/{}/val'.format(x) for x in attacks if x is not attacks[unknown_attack_idx]]\n",
    "\n",
    "print('Train unlabel paths: ', train_unlabel_paths)\n",
    "print('Train label paths: ', train_label_paths)\n",
    "print('Validation paths: ', val_paths)\n",
    "\n",
    "data_info_paths = ['./Data/{}/datainfo.txt'.format(x) for x in attacks if x is not attacks[unknown_attack_idx]]\n",
    "for f in data_info_paths:\n",
    "    data_read = json.load(open(f))\n",
    "    for key in data_info.keys():\n",
    "        data_info[key] += data_read[key]\n",
    "        \n",
    "print('Data info:', data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dfec498-1786-4a05-b1db-6f0d318a103a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown attack:  DoS\n",
      "Data info:  {'train_unlabel': 1662780, 'train_label': 184754, 'validation': 461884, 'test': 1099723}\n"
     ]
    }
   ],
   "source": [
    "print('Unknown attack: ', attacks[unknown_attack_idx])\n",
    "data_info_unknown_attack = json.load(open('./Data/{}/datainfo.txt'.format(attacks[unknown_attack_idx])))\n",
    "val_unknown_attack_path = './Data/{}/val'.format(attacks[unknown_attack_idx])\n",
    "print('Data info: ', data_info_unknown_attack)\n",
    "validation_unknown_size = data_info_unknown_attack['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eae55b-86cb-4d3d-967c-8773b2381a63",
   "metadata": {},
   "source": [
    "## Idea to improve:\n",
    "\n",
    "1) Tune n_l1, n_l2 and z_dim \n",
    "2) Generator, Regualrization and Semi phase with different learning rate\n",
    "3) Discriminator wih smaller learning rate\n",
    "4) Consider about regularize autoencoder: parse penalty or variational "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6edf13-babf-4be5-8d1d-cf1e94fd9276",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d2c964e-d75c-4945-867d-750f6a17d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 29 * 29\n",
    "n_l1 = 1000\n",
    "n_l2 = 1000\n",
    "z_dim = 10\n",
    "batch_size = 100\n",
    "n_epochs = 500\n",
    "# learning_rate = 0.001\n",
    "supervised_lr = 0.001\n",
    "reconstruction_lr = 0.0006\n",
    "regularization_lr = 0.0008\n",
    "beta1 = 0.9\n",
    "n_labels = 2\n",
    "n_labeled = data_info['train_label']\n",
    "validation_size = data_info['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6bc6b2-4ee5-4a35-8435-efb91abaf094",
   "metadata": {},
   "source": [
    "# Define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef49e29-fbb8-46a4-a42b-8477258db622",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For unknown attack\n",
    "train_unlabel = data_from_tfrecord(train_unlabel_paths, batch_size, n_epochs)\n",
    "train_label = data_from_tfrecord(train_label_paths, batch_size, n_epochs)\n",
    "validation = data_from_tfrecord(val_paths, batch_size, n_epochs)\n",
    "validation_unknown = data_from_tfrecord(val_unknown_attack_path, batch_size, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "721812cc-3384-46a4-8580-a237702408b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thiennu/Research/IDS/utils.py:21: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "# For all\n",
    "train_unlabel = data_from_tfrecord(glob.glob(data_path + 'train_unlabel'), batch_size, n_epochs)\n",
    "train_label = data_from_tfrecord(glob.glob(data_path + 'train_label'), batch_size, n_epochs)\n",
    "validation = data_from_tfrecord(glob.glob(data_path + 'val'), batch_size, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde0cd17-2bd4-4610-92e2-04659346d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = data_from_tfrecord('./Data/TFRecord/Normal_DoS', 100, 1)\n",
    "# init = tf.global_variables_initializer()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     x_l, y_l = data_helper(train_unlabel, sess)\n",
    "#     print(x_l.shape, y_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e582de4-6eba-4747-b60a-e717a21beff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for input data and the targets\n",
    "x_input = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Input')\n",
    "x_input_l = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Labeled_Input')\n",
    "y_input = tf.placeholder(dtype=tf.float32, shape=[batch_size, n_labels], name='Labels')\n",
    "x_target = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Target')\n",
    "real_distribution = tf.placeholder(dtype=tf.float32, shape=[batch_size, z_dim], name='Real_distribution')\n",
    "categorial_distribution = tf.placeholder(dtype=tf.float32, shape=[batch_size, n_labels],\n",
    "                                         name='Categorical_distribution')\n",
    "manual_decoder_input = tf.placeholder(dtype=tf.float32, shape=[1, z_dim + n_labels], name='Decoder_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56580a6-906a-4566-8611-395a9e1c2f06",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ba74bc-84bf-436d-8c70-6164019e2648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thiennu/Research/IDS/model.py:26: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reconstruction Phase\n",
    "# Encoder try to predict both label and latent space of the input, which will be feed into Decoder to reconstruct the input\n",
    "# The process is optimized by autoencoder_loss which is the MSE of the decoder_output and the orginal input\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    encoder_output_label, encoder_output_latent = encoder(x_input)\n",
    "    decoder_input = tf.concat([encoder_output_label, encoder_output_latent], 1)\n",
    "    decoder_output = decoder(decoder_input)\n",
    "\n",
    "autoencoder_loss = tf.reduce_mean(tf.square(x_target - decoder_output))\n",
    "autoencoder_optimizer = tf.train.AdamOptimizer(learning_rate=reconstruction_lr, beta1=beta1).minimize(autoencoder_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f654842-b99b-45c1-8bc1-bfcbabd9d983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Regularization Phase\n",
    "# Train both 2 discriminator of gaussian and categorical to detect the output from encoder\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    # Discriminator for gaussian\n",
    "    d_g_real = discriminator_gauss(real_distribution)\n",
    "    d_g_fake = discriminator_gauss(encoder_output_latent, reuse=True)\n",
    "# Need to seperate dicriminator of gaussian and categorical\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    # Discrimnator for categorical\n",
    "    d_c_real = discriminator_categorical(categorial_distribution)\n",
    "    d_c_fake = discriminator_categorical(encoder_output_label, reuse=True)\n",
    "\n",
    "# Discriminator gaussian loss \n",
    "dc_g_loss_real = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_g_real), logits=d_g_real))\n",
    "dc_g_loss_fake = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_g_fake), logits=d_g_fake))\n",
    "dc_g_loss = dc_g_loss_real + dc_g_loss_fake\n",
    "\n",
    "# Discriminator categorical loss\n",
    "dc_c_loss_real = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_c_real), logits=d_c_real))\n",
    "dc_c_loss_fake = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_c_fake), logits=d_c_fake))\n",
    "dc_c_loss = dc_c_loss_fake + dc_c_loss_real\n",
    "\n",
    "all_variables = tf.trainable_variables()\n",
    "dc_g_var = [var for var in all_variables if 'dc_g_' in var.name]\n",
    "dc_c_var = [var for var in all_variables if 'dc_c_' in var.name]\n",
    "discriminator_g_optimizer = tf.train.AdamOptimizer(learning_rate=regularization_lr/5,\n",
    "                                                       beta1=beta1).minimize(dc_g_loss, var_list=dc_g_var)\n",
    "discriminator_c_optimizer = tf.train.AdamOptimizer(learning_rate=regularization_lr/5,\n",
    "                                                       beta1=beta1).minimize(dc_c_loss, var_list=dc_c_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bddba91-3073-409d-aec0-7b90e4bc28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator loss\n",
    "generator_g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_g_fake), logits=d_g_fake))\n",
    "generator_c_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_c_fake), logits=d_c_fake))\n",
    "generator_loss = generator_g_loss + generator_c_loss\n",
    "\n",
    "en_var = [var for var in all_variables if 'e_' in var.name]\n",
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate=regularization_lr, beta1=beta1).minimize(generator_loss, var_list=en_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81da02c3-59e6-426c-b286-2765a1c64483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-fdf0ca0559fd>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Semi-Supervised Classification Phase\n",
    "# Train encoder with a small amount of label samples\n",
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    encoder_output_label_, _ = encoder(x_input_l, reuse=True, supervised=True)\n",
    "    \n",
    "# Classification accuracy of encoder\n",
    "output_label = tf.argmax(encoder_output_label_, 1)\n",
    "correct_pred = tf.equal(output_label, tf.argmax(y_input, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "supervised_encoder_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=encoder_output_label_))\n",
    "supervised_encoder_optimizer = tf.train.AdamOptimizer(learning_rate=supervised_lr, beta1=beta1).minimize(supervised_encoder_loss, var_list=en_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc99cfcc-4cc3-4675-94d4-8e01facb63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_results():\n",
    "    \"\"\"\n",
    "    Forms folders for each run to store the tensorboard files, saved models and the log files.\n",
    "    :return: three string pointing to tensorboard, saved models and log paths respectively.\n",
    "    \"\"\"\n",
    "    folder_name = \"/{0}_{1}_{2}_{3}_{4}_{5}_Semi_Supervised\". \\\n",
    "        format(datetime.datetime.now(), z_dim, supervised_lr, batch_size, n_epochs, beta1)\n",
    "    tensorboard_path = results_path + folder_name + '/Tensorboard'\n",
    "    saved_model_path = results_path + folder_name + '/Saved_models/'\n",
    "    log_path = results_path + folder_name + '/log'\n",
    "    if not os.path.exists(results_path + folder_name):\n",
    "        os.mkdir(results_path + folder_name)\n",
    "        os.mkdir(tensorboard_path)\n",
    "        os.mkdir(saved_model_path)\n",
    "        os.mkdir(log_path)\n",
    "    return tensorboard_path, saved_model_path, log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790fc5f2-157c-45c3-9a97-41df4a341adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(x, y, batch_size):\n",
    "    \"\"\"\n",
    "    Used to return a random batch from the given inputs.\n",
    "    :param x: Input images of shape [None, 784]\n",
    "    :param y: Input labels of shape [None, 10]\n",
    "    :param batch_size: integer, batch size of images and labels to return\n",
    "    :return: x -> [batch_size, 784], y-> [batch_size, 10]\n",
    "    \"\"\"\n",
    "    index = np.arange(n_labeled)\n",
    "    random_index = np.random.permutation(index)[:batch_size]\n",
    "    return x[random_index], y[random_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c47771-919c-4662-b6ab-69836feb9f69",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2c1490b-e4bd-4060-8fb6-805d7c26eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_acc(val_size, batch_size, tfdata):\n",
    "    acc = 0\n",
    "    y_true, y_pred = [], []\n",
    "    num_batches = int(val_size/batch_size)\n",
    "    for j in tqdm.tqdm(range(num_batches)):\n",
    "        batch_x_l, batch_y_l = data_helper(tfdata, sess)\n",
    "        batch_pred = sess.run(output_label, feed_dict={x_input_l: batch_x_l, y_input: batch_y_l})\n",
    "        \n",
    "        #batch_pred = sess.run(pred_label, feed_dict={x_input_l: x_test})\n",
    "        batch_label = np.argmax(batch_y_l, axis=1)\n",
    "        y_pred += batch_pred.tolist()\n",
    "        y_true += batch_label.tolist()\n",
    "        \n",
    "    avg_acc = np.equal(y_true, y_pred).mean()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    fnr = fn/(tp + fn)\n",
    "    err = (fn + fp) / (tp + tn + fp + fn)\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = 1 - fnr\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    #print(avg_acc, precision, recall, f1)\n",
    "    return avg_acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58133466-3c86-4fe9-a0ec-e8e3cd294750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Autoencoder Loss is illegal; using Autoencoder_Loss instead.\n",
      "INFO:tensorflow:Summary name Discriminator gauss Loss is illegal; using Discriminator_gauss_Loss instead.\n",
      "INFO:tensorflow:Summary name Discriminator categorical Loss is illegal; using Discriminator_categorical_Loss instead.\n",
      "INFO:tensorflow:Summary name Generator Loss is illegal; using Generator_Loss instead.\n",
      "INFO:tensorflow:Summary name Supervised Encoder Loss is illegal; using Supervised_Encoder_Loss instead.\n",
      "INFO:tensorflow:Summary name Encoder Gauss Distribution is illegal; using Encoder_Gauss_Distribution instead.\n",
      "INFO:tensorflow:Summary name Real Gauss Distribution is illegal; using Real_Gauss_Distribution instead.\n",
      "INFO:tensorflow:Summary name Encoder Categorical Distribution is illegal; using Encoder_Categorical_Distribution instead.\n",
      "INFO:tensorflow:Summary name Real Categorical Distribution is illegal; using Real_Categorical_Distribution instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Epoch 0/500------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 7536/8000 [01:54<00:05, 89.76it/s]"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Tensorboard visualization\n",
    "tf.summary.scalar(name='Autoencoder Loss', tensor=autoencoder_loss)\n",
    "tf.summary.scalar(name='Discriminator gauss Loss', tensor=dc_g_loss)\n",
    "tf.summary.scalar(name='Discriminator categorical Loss', tensor=dc_c_loss)\n",
    "tf.summary.scalar(name='Generator Loss', tensor=generator_loss)\n",
    "tf.summary.scalar(name='Supervised Encoder Loss', tensor=supervised_encoder_loss)\n",
    "# tf.summary.scalar(name='Supervised Encoder Accuracy', tensor=accuracy)\n",
    "tf.summary.histogram(name='Encoder Gauss Distribution', values=encoder_output_latent)\n",
    "tf.summary.histogram(name='Real Gauss Distribution', values=real_distribution)\n",
    "tf.summary.histogram(name='Encoder Categorical Distribution', values=encoder_output_label)\n",
    "tf.summary.histogram(name='Real Categorical Distribution', values=categorial_distribution)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "accuracies = []\n",
    "# Saving the model\n",
    "saver = tf.train.Saver()\n",
    "step = 0\n",
    "# Early stopping\n",
    "save_sess = None\n",
    "best_acc = 1\n",
    "stop = False\n",
    "last_improvement = 0\n",
    "require_improvement = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if True:\n",
    "        tensorboard_path, saved_model_path, log_path = form_results()\n",
    "        sess.run(init)\n",
    "        writer = tf.summary.FileWriter(logdir=tensorboard_path, graph=sess.graph)\n",
    "        for epoch in range(n_epochs):\n",
    "            if epoch == 150:\n",
    "                supervised_lr /= 10\n",
    "                reconstruction_lr /= 10\n",
    "                regularization_lr /= 10\n",
    "            n_batches = int(n_labeled / batch_size)\n",
    "            print(\"------------------Epoch {}/{}------------------\".format(epoch, n_epochs))\n",
    "            for b in tqdm.tqdm(range(1, n_batches + 1)):\n",
    "                z_real_dist = np.random.randn(batch_size, z_dim) * 5.\n",
    "                real_cat_dist = np.random.randint(low=0, high=2, size=batch_size)\n",
    "                real_cat_dist = np.eye(n_labels)[real_cat_dist]\n",
    "                \n",
    "                batch_x_ul, _ = data_stream(train_unlabel, sess)\n",
    "                batch_x_l, batch_y_l = data_stream(train_label, sess)\n",
    "                \n",
    "                sess.run(autoencoder_optimizer, feed_dict={x_input: batch_x_ul, x_target: batch_x_ul})\n",
    "                sess.run(discriminator_g_optimizer,\n",
    "                         feed_dict={x_input: batch_x_ul, x_target: batch_x_ul, real_distribution: z_real_dist})\n",
    "                sess.run(discriminator_c_optimizer,\n",
    "                         feed_dict={x_input: batch_x_ul, x_target: batch_x_ul,\n",
    "                                    categorial_distribution: real_cat_dist})\n",
    "                sess.run(generator_optimizer, feed_dict={x_input: batch_x_ul, x_target: batch_x_ul})\n",
    "                \n",
    "                if b % 10 == 0:\n",
    "                    sess.run(supervised_encoder_optimizer, feed_dict={x_input_l: batch_x_l, y_input: batch_y_l})\n",
    "                if b % 100 == 0:\n",
    "                    a_loss, d_g_loss, d_c_loss, g_loss, s_loss, summary = sess.run(\n",
    "                        [autoencoder_loss, dc_g_loss, dc_c_loss, generator_loss, supervised_encoder_loss,\n",
    "                         summary_op],\n",
    "                        feed_dict={x_input: batch_x_ul, x_target: batch_x_ul,\n",
    "                                   real_distribution: z_real_dist, y_input: batch_y_l, x_input_l: batch_x_l,\n",
    "                                   categorial_distribution: real_cat_dist})\n",
    "                    writer.add_summary(summary, global_step=step)\n",
    "                    with open(log_path + '/log.txt', 'a') as log:\n",
    "                        log.write(\"Epoch: {}, iteration: {}\\n\".format(epoch, b))\n",
    "                        log.write(\"Autoencoder Loss: {}\\n\".format(a_loss))\n",
    "                        log.write(\"Discriminator Gauss Loss: {}\".format(d_g_loss))\n",
    "                        log.write(\"Discriminator Categorical Loss: {}\".format(d_c_loss))\n",
    "                        log.write(\"Generator Loss: {}\\n\".format(g_loss))\n",
    "                        log.write(\"Supervised Loss: {}\".format(s_loss))\n",
    "                step += 1\n",
    "                \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(\"Runing on validation...\")\n",
    "                acc_known, precision_known, recall_known, f1_known = get_val_acc(validation_size, batch_size, validation)\n",
    "                print(\"Accuracy on Known attack: {}\".format(acc_known))\n",
    "                print(\"Precision on Known attack: {}\".format(precision_known))\n",
    "                print(\"Recall on Known attack: {}\".format(recall_known))\n",
    "                print(\"F1 on Known attack: {}\".format(f1_known))\n",
    "            saver.save(sess, save_path=saved_model_path, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f641feb-0e3c-466d-9ea3-e8dc490deea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow1.x] *",
   "language": "python",
   "name": "conda-env-Tensorflow1.x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
