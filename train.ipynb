{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f2229d-8c22-4e4d-b499-d3f42ca446a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db494cdf-d859-49b2-820f-4389299e270f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_unlabel': 8166200,\n",
       " 'train_label': 907358,\n",
       " 'validation': 2268392,\n",
       " 'test': 4970811}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this when train with all data\n",
    "data_info = {\n",
    "   \"train_unlabel\": 0, \n",
    "    \"train_label\": 0, \n",
    "    \"validation\": 0, \n",
    "    \"test\": 0\n",
    "}\n",
    "for f in glob.glob('./Data/*/datainfo.txt'):\n",
    "    data_read = json.load(open(f))\n",
    "    for key in data_info.keys():\n",
    "        data_info[key] += data_read[key]\n",
    "        \n",
    "data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eae55b-86cb-4d3d-967c-8773b2381a63",
   "metadata": {},
   "source": [
    "## Idea to improve:\n",
    "\n",
    "1) Tune n_l1, n_l2 and z_dim\n",
    "2) Generator, Regualrization and Semi phase with different learning rate\n",
    "3) Discriminator wih smaller learning rate\n",
    "4) Consider about regularize autoencoder: parse penalty or variational "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760f0d91-442d-456b-aa19-b5a89bf07a2f",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b512c777-302b-4921-a169-646f877e8d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data info:  {'train_unlabel': 8166200, 'train_label': 907358, 'validation': 2268392, 'test': 4970811}\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "# Put the types of attack here\n",
    "attack = 'all' # DoS, Fuzzy, gear, RPM, all\n",
    "results_path = './Results/{}/'.format(attack)\n",
    "data_path = './Data/*/'\n",
    "# data_path = './Data/{}/'.format(attack)\n",
    "# data_info = json.load(open(data_path + 'datainfo.txt'))\n",
    "print('Data info: ', data_info)\n",
    "input_dim = 29 * 29\n",
    "n_l1 = 1000\n",
    "n_l2 = 1000\n",
    "z_dim = 10\n",
    "batch_size = 100\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "n_labels = 2\n",
    "n_labeled = data_info['train_label']\n",
    "validation_size = data_info['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f411497-1add-4c92-b02f-08c8600ce0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(x, n1, n2, name):\n",
    "    \"\"\"\n",
    "    Used to create a dense layer.\n",
    "    :param x: input tensor to the dense layer\n",
    "    :param n1: no. of input neurons\n",
    "    :param n2: no. of output neurons\n",
    "    :param name: name of the entire dense layer.i.e, variable scope name.\n",
    "    :return: tensor with shape [batch_size, n2]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name, reuse=None):\n",
    "        weights = tf.get_variable(\"weights\", shape=[n1, n2],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0., stddev=0.01))\n",
    "        bias = tf.get_variable(\"bias\", shape=[n2], initializer=tf.constant_initializer(0.0))\n",
    "        out = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61811a1f-6af2-4286-93ed-7bca40b10d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, reuse=False, supervised=False):\n",
    "    \"\"\"\n",
    "    Encode part of the autoencoder.\n",
    "    :param x: input to the autoencoder\n",
    "    :param reuse: True -> Reuse the encoder variables, False -> Create or search of variables before creating\n",
    "    :param supervised: True -> returns output without passing it through softmax,\n",
    "                       False -> returns output after passing it through softmax.\n",
    "    :return: tensor which is the classification output and a hidden latent variable of the autoencoder.\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Encoder'):\n",
    "        e_dense_1 = tf.nn.relu(dense(x, input_dim, n_l1, 'e_dense_1'))\n",
    "        e_dense_2 = tf.nn.relu(dense(e_dense_1, n_l1, n_l2, 'e_dense_2'))\n",
    "        latent_variable = dense(e_dense_2, n_l2, z_dim, 'e_latent_variable')\n",
    "        cat_op = dense(e_dense_2, n_l2, n_labels, 'e_label')\n",
    "        if not supervised:\n",
    "            softmax_label = tf.nn.softmax(logits=cat_op, name='e_softmax_label')\n",
    "        else:\n",
    "            softmax_label = cat_op\n",
    "        return softmax_label, latent_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b3d9b6-d8d4-46ad-9a50-f082a0dc80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x, reuse=False):\n",
    "    \"\"\"\n",
    "    Decoder part of the autoencoder.\n",
    "    :param x: input to the decoder\n",
    "    :param reuse: True -> Reuse the decoder variables, False -> Create or search of variables before creating\n",
    "    :return: tensor which should ideally be the input given to the encoder.\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Decoder'):\n",
    "        d_dense_1 = tf.nn.relu(dense(x, z_dim + n_labels, n_l2, 'd_dense_1'))\n",
    "        d_dense_2 = tf.nn.relu(dense(d_dense_1, n_l2, n_l1, 'd_dense_2'))\n",
    "        output = tf.nn.sigmoid(dense(d_dense_2, n_l1, input_dim, 'd_output'))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c13963-4fb0-4dad-af12-de32f88d251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_gauss(x, reuse=False):\n",
    "    \"\"\"\n",
    "    Discriminator that is used to match the posterior distribution with a given gaussian distribution.\n",
    "    :param x: tensor of shape [batch_size, z_dim]\n",
    "    :param reuse: True -> Reuse the discriminator variables,\n",
    "                  False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Discriminator_Gauss'):\n",
    "        dc_den1 = tf.nn.relu(dense(x, z_dim, n_l1, name='dc_g_den1'))\n",
    "        dc_den2 = tf.nn.relu(dense(dc_den1, n_l1, n_l2, name='dc_g_den2'))\n",
    "        output = dense(dc_den2, n_l2, 1, name='dc_g_output')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68fc9c8c-5774-4ec0-9d7a-f0639900b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_categorical(x, reuse=False):\n",
    "    \"\"\"\n",
    "    Discriminator that is used to match the posterior distribution with a given categorical distribution.\n",
    "    :param x: tensor of shape [batch_size, n_labels]\n",
    "    :param reuse: True -> Reuse the discriminator variables,\n",
    "                  False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    with tf.name_scope('Discriminator_Categorial'):\n",
    "        dc_den1 = tf.nn.relu(dense(x, n_labels, n_l1, name='dc_c_den1'))\n",
    "        dc_den2 = tf.nn.relu(dense(dc_den1, n_l1, n_l2, name='dc_c_den2'))\n",
    "        output = dense(dc_den2, n_l2, 1, name='dc_c_output')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6bc6b2-4ee5-4a35-8435-efb91abaf094",
   "metadata": {},
   "source": [
    "# Define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374ae689-e605-4c5f-a871-86fd1fa50656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(example):\n",
    "    feature_description = {\n",
    "    'input_features': tf.io.FixedLenFeature([input_dim], tf.int64),\n",
    "    'label': tf.io.FixedLenFeature([1], tf.int64)\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb64ae5-56b8-42a3-8f81-11728d754065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_tfrecord(tf_filepath, batch_size, repeat_time):\n",
    "    data = tf.data.TFRecordDataset(tf_filepath)\n",
    "    data = data.map(read_tfrecord)\n",
    "    data = data.shuffle(1)\n",
    "    data = data.repeat(repeat_time + 1)\n",
    "    data = data.batch(batch_size)\n",
    "    # print(tf.data.experimental.cardinality(data))\n",
    "    iterator = data.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721812cc-3384-46a4-8580-a237702408b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-482305dc5716>:8: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "# For all\n",
    "train_unlabel = data_from_tfrecord(glob.glob(data_path + 'train_unlabel'), batch_size, n_epochs)\n",
    "train_label = data_from_tfrecord(glob.glob(data_path + 'train_label'), batch_size, n_epochs)\n",
    "validation = data_from_tfrecord(glob.glob(data_path + 'val'), batch_size, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f58c55c-4b4c-4d72-ab4f-f41d6e64097c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-482305dc5716>:8: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "#For each attack\n",
    "train_unlabel = data_from_tfrecord(data_path + 'train_unlabel', batch_size, n_epochs)\n",
    "train_label = data_from_tfrecord(data_path + 'train_label', n_labeled, 1)\n",
    "validation = data_from_tfrecord(data_path + 'val', batch_size, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "053c1939-a40a-4023-bb82-a8d184e8f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_helper(data_tf, sess):\n",
    "    data = sess.run(data_tf)\n",
    "    x, y = data['input_features'], data['label']\n",
    "    size = x.shape[0]\n",
    "    y_one_hot = np.eye(n_labels)[y].reshape([size, n_labels])\n",
    "    return x, y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde0cd17-2bd4-4610-92e2-04659346d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init = tf.global_variables_initializer()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     x_l, y_l = data_helper(train, sess)\n",
    "#     print(x_l.shape, y_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e582de4-6eba-4747-b60a-e717a21beff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for input data and the targets\n",
    "x_input = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Input')\n",
    "x_input_l = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Labeled_Input')\n",
    "y_input = tf.placeholder(dtype=tf.float32, shape=[batch_size, n_labels], name='Labels')\n",
    "x_target = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Target')\n",
    "real_distribution = tf.placeholder(dtype=tf.float32, shape=[batch_size, z_dim], name='Real_distribution')\n",
    "categorial_distribution = tf.placeholder(dtype=tf.float32, shape=[batch_size, n_labels],\n",
    "                                         name='Categorical_distribution')\n",
    "manual_decoder_input = tf.placeholder(dtype=tf.float32, shape=[1, z_dim + n_labels], name='Decoder_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56580a6-906a-4566-8611-395a9e1c2f06",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55ba74bc-84bf-436d-8c70-6164019e2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction Phase\n",
    "# Encoder try to predict both label and latent space of the input, which will be feed into Decoder to reconstruct the input\n",
    "# The process is optimized by autoencoder_loss which is the MSE of the decoder_output and the orginal input\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    encoder_output_label, encoder_output_latent = encoder(x_input)\n",
    "    decoder_input = tf.concat([encoder_output_label, encoder_output_latent], 1)\n",
    "    decoder_output = decoder(decoder_input)\n",
    "\n",
    "autoencoder_loss = tf.reduce_mean(tf.square(x_target - decoder_output))\n",
    "autoencoder_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(autoencoder_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f654842-b99b-45c1-8bc1-bfcbabd9d983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Regularization Phase\n",
    "# Train both 2 discriminator of gaussian and categorical to detect the output from encoder\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    # Discriminator for gaussian\n",
    "    d_g_real = discriminator_gauss(real_distribution)\n",
    "    d_g_fake = discriminator_gauss(encoder_output_latent, reuse=True)\n",
    "# Need to seperate dicriminator of gaussian and categorical\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    # Discrimnator for categorical\n",
    "    d_c_real = discriminator_categorical(categorial_distribution)\n",
    "    d_c_fake = discriminator_categorical(encoder_output_label, reuse=True)\n",
    "\n",
    "# Discriminator gaussian loss \n",
    "dc_g_loss_real = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_g_real), logits=d_g_real))\n",
    "dc_g_loss_fake = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_g_fake), logits=d_g_fake))\n",
    "dc_g_loss = dc_g_loss_real + dc_g_loss_fake\n",
    "\n",
    "# Discriminator categorical loss\n",
    "dc_c_loss_real = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_c_real), logits=d_c_real))\n",
    "dc_c_loss_fake = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_c_fake), logits=d_c_fake))\n",
    "dc_c_loss = dc_c_loss_fake + dc_c_loss_real\n",
    "\n",
    "all_variables = tf.trainable_variables()\n",
    "dc_g_var = [var for var in all_variables if 'dc_g_' in var.name]\n",
    "dc_c_var = [var for var in all_variables if 'dc_c_' in var.name]\n",
    "discriminator_g_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                                       beta1=beta1).minimize(dc_g_loss, var_list=dc_g_var)\n",
    "discriminator_c_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                                       beta1=beta1).minimize(dc_c_loss, var_list=dc_c_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bddba91-3073-409d-aec0-7b90e4bc28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator loss\n",
    "generator_g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_g_fake), logits=d_g_fake))\n",
    "generator_c_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_c_fake), logits=d_c_fake))\n",
    "generator_loss = generator_g_loss + generator_c_loss\n",
    "\n",
    "en_var = [var for var in all_variables if 'e_' in var.name]\n",
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(generator_loss, var_list=en_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81da02c3-59e6-426c-b286-2765a1c64483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-3455a15bfac4>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Semi-Supervised Classification Phase\n",
    "# Train encoder with a small amount of label samples\n",
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    encoder_output_label_, _ = encoder(x_input_l, reuse=True, supervised=True)\n",
    "    \n",
    "# Classification accuracy of encoder\n",
    "correct_pred = tf.equal(tf.argmax(encoder_output_label_, 1), tf.argmax(y_input, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "supervised_encoder_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=encoder_output_label_))\n",
    "supervised_encoder_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1).minimize(supervised_encoder_loss, var_list=en_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc99cfcc-4cc3-4675-94d4-8e01facb63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_results():\n",
    "    \"\"\"\n",
    "    Forms folders for each run to store the tensorboard files, saved models and the log files.\n",
    "    :return: three string pointing to tensorboard, saved models and log paths respectively.\n",
    "    \"\"\"\n",
    "    folder_name = \"/{0}_{1}_{2}_{3}_{4}_{5}_Semi_Supervised\". \\\n",
    "        format(datetime.datetime.now(), z_dim, learning_rate, batch_size, n_epochs, beta1)\n",
    "    tensorboard_path = results_path + folder_name + '/Tensorboard'\n",
    "    saved_model_path = results_path + folder_name + '/Saved_models/'\n",
    "    log_path = results_path + folder_name + '/log'\n",
    "    if not os.path.exists(results_path + folder_name):\n",
    "        os.mkdir(results_path + folder_name)\n",
    "        os.mkdir(tensorboard_path)\n",
    "        os.mkdir(saved_model_path)\n",
    "        os.mkdir(log_path)\n",
    "    return tensorboard_path, saved_model_path, log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "790fc5f2-157c-45c3-9a97-41df4a341adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(x, y, batch_size):\n",
    "    \"\"\"\n",
    "    Used to return a random batch from the given inputs.\n",
    "    :param x: Input images of shape [None, 784]\n",
    "    :param y: Input labels of shape [None, 10]\n",
    "    :param batch_size: integer, batch size of images and labels to return\n",
    "    :return: x -> [batch_size, 784], y-> [batch_size, 10]\n",
    "    \"\"\"\n",
    "    index = np.arange(n_labeled)\n",
    "    random_index = np.random.permutation(index)[:batch_size]\n",
    "    return x[random_index], y[random_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c47771-919c-4662-b6ab-69836feb9f69",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58133466-3c86-4fe9-a0ec-e8e3cd294750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Autoencoder Loss is illegal; using Autoencoder_Loss instead.\n",
      "INFO:tensorflow:Summary name Discriminator gauss Loss is illegal; using Discriminator_gauss_Loss instead.\n",
      "INFO:tensorflow:Summary name Discriminator categorical Loss is illegal; using Discriminator_categorical_Loss instead.\n",
      "INFO:tensorflow:Summary name Generator Loss is illegal; using Generator_Loss instead.\n",
      "INFO:tensorflow:Summary name Supervised Encoder Loss is illegal; using Supervised_Encoder_Loss instead.\n",
      "INFO:tensorflow:Summary name Encoder Gauss Distribution is illegal; using Encoder_Gauss_Distribution instead.\n",
      "INFO:tensorflow:Summary name Real Gauss Distribution is illegal; using Real_Gauss_Distribution instead.\n",
      "INFO:tensorflow:Summary name Encoder Categorical Distribution is illegal; using Encoder_Categorical_Distribution instead.\n",
      "INFO:tensorflow:Summary name Real Categorical Distribution is illegal; using Real_Categorical_Distribution instead.\n",
      "------------------Epoch 0/1000------------------\n",
      "------------------Epoch 1/1000------------------\n",
      "------------------Epoch 2/1000------------------\n",
      "------------------Epoch 3/1000------------------\n",
      "------------------Epoch 4/1000------------------\n",
      "Encoder Classification Accuracy: 0.8651598146635194\n",
      "------------------Epoch 5/1000------------------\n",
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "------------------Epoch 6/1000------------------\n",
      "------------------Epoch 7/1000------------------\n",
      "------------------Epoch 8/1000------------------\n",
      "------------------Epoch 9/1000------------------\n",
      "Encoder Classification Accuracy: 0.9335581766535817\n",
      "------------------Epoch 10/1000------------------\n",
      "------------------Epoch 11/1000------------------\n",
      "------------------Epoch 12/1000------------------\n",
      "------------------Epoch 13/1000------------------\n",
      "------------------Epoch 14/1000------------------\n",
      "Encoder Classification Accuracy: 0.9361420516267117\n",
      "------------------Epoch 15/1000------------------\n",
      "------------------Epoch 16/1000------------------\n",
      "------------------Epoch 17/1000------------------\n",
      "------------------Epoch 18/1000------------------\n",
      "------------------Epoch 19/1000------------------\n",
      "Encoder Classification Accuracy: 0.9315637313530768\n",
      "------------------Epoch 20/1000------------------\n",
      "------------------Epoch 21/1000------------------\n",
      "------------------Epoch 22/1000------------------\n",
      "------------------Epoch 23/1000------------------\n",
      "------------------Epoch 24/1000------------------\n",
      "Encoder Classification Accuracy: 0.9296790616819112\n",
      "------------------Epoch 25/1000------------------\n",
      "------------------Epoch 26/1000------------------\n",
      "------------------Epoch 27/1000------------------\n",
      "------------------Epoch 28/1000------------------\n",
      "------------------Epoch 29/1000------------------\n",
      "Encoder Classification Accuracy: 0.9823325925212223\n",
      "------------------Epoch 30/1000------------------\n",
      "------------------Epoch 31/1000------------------\n",
      "------------------Epoch 32/1000------------------\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Tensorboard visualization\n",
    "tf.summary.scalar(name='Autoencoder Loss', tensor=autoencoder_loss)\n",
    "tf.summary.scalar(name='Discriminator gauss Loss', tensor=dc_g_loss)\n",
    "tf.summary.scalar(name='Discriminator categorical Loss', tensor=dc_c_loss)\n",
    "tf.summary.scalar(name='Generator Loss', tensor=generator_loss)\n",
    "tf.summary.scalar(name='Supervised Encoder Loss', tensor=supervised_encoder_loss)\n",
    "# tf.summary.scalar(name='Supervised Encoder Accuracy', tensor=accuracy)\n",
    "tf.summary.histogram(name='Encoder Gauss Distribution', values=encoder_output_latent)\n",
    "tf.summary.histogram(name='Real Gauss Distribution', values=real_distribution)\n",
    "tf.summary.histogram(name='Encoder Categorical Distribution', values=encoder_output_label)\n",
    "tf.summary.histogram(name='Real Categorical Distribution', values=categorial_distribution)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "accuracies = []\n",
    "# Saving the model\n",
    "saver = tf.train.Saver()\n",
    "step = 0\n",
    "# Early stopping\n",
    "save_sess = None\n",
    "best_acc = 1\n",
    "stop = False\n",
    "last_improvement = 0\n",
    "require_improvement = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if True:\n",
    "        tensorboard_path, saved_model_path, log_path = form_results()\n",
    "        sess.run(init)\n",
    "        writer = tf.summary.FileWriter(logdir=tensorboard_path, graph=sess.graph)\n",
    "        # x_l, y_l = mnist.test.next_batch(n_labeled)\n",
    "        #x_l, y_l = data_helper(train_label, sess)\n",
    "        for epoch in range(n_epochs):\n",
    "            # Eearly stopping\n",
    "            if stop:\n",
    "                print('Eearly stopping')\n",
    "                break\n",
    "            n_batches = int(n_labeled / batch_size)\n",
    "            print(\"------------------Epoch {}/{}------------------\".format(epoch, n_epochs))\n",
    "            for b in range(1, n_batches + 1):\n",
    "                z_real_dist = np.random.randn(batch_size, z_dim) * 5.\n",
    "                real_cat_dist = np.random.randint(low=0, high=2, size=batch_size)\n",
    "                real_cat_dist = np.eye(n_labels)[real_cat_dist]\n",
    "                # batch_x_ul, _ = mnist.train.next_batch(batch_size)\n",
    "                batch_x_ul, _ = data_helper(train_unlabel, sess)\n",
    "                batch_x_l, batch_y_l = data_helper(train_label, sess)\n",
    "                \n",
    "                sess.run(autoencoder_optimizer, feed_dict={x_input: batch_x_ul, x_target: batch_x_ul})\n",
    "                sess.run(discriminator_g_optimizer,\n",
    "                         feed_dict={x_input: batch_x_ul, x_target: batch_x_ul, real_distribution: z_real_dist})\n",
    "                sess.run(discriminator_c_optimizer,\n",
    "                         feed_dict={x_input: batch_x_ul, x_target: batch_x_ul,\n",
    "                                    categorial_distribution: real_cat_dist})\n",
    "                sess.run(generator_optimizer, feed_dict={x_input: batch_x_ul, x_target: batch_x_ul})\n",
    "                \n",
    "                sess.run(supervised_encoder_optimizer, feed_dict={x_input_l: batch_x_l, y_input: batch_y_l})\n",
    "                if b % 100 == 0:\n",
    "                    a_loss, d_g_loss, d_c_loss, g_loss, s_loss, summary = sess.run(\n",
    "                        [autoencoder_loss, dc_g_loss, dc_c_loss, generator_loss, supervised_encoder_loss,\n",
    "                         summary_op],\n",
    "                        feed_dict={x_input: batch_x_ul, x_target: batch_x_ul,\n",
    "                                   real_distribution: z_real_dist, y_input: batch_y_l, x_input_l: batch_x_l,\n",
    "                                   categorial_distribution: real_cat_dist})\n",
    "                    writer.add_summary(summary, global_step=step)\n",
    "#                     print(\"Epoch: {}, iteration: {}\".format(i, b))\n",
    "#                     print(\"Autoencoder Loss: {}\".format(a_loss))\n",
    "#                     print(\"Discriminator Gauss Loss: {}\".format(d_g_loss))\n",
    "#                     print(\"Discriminator Categorical Loss: {}\".format(d_c_loss))\n",
    "#                     print(\"Generator Loss: {}\".format(g_loss))\n",
    "#                     print(\"Supervised Loss: {}\\n\".format(s_loss))\n",
    "                    with open(log_path + '/log.txt', 'a') as log:\n",
    "                        log.write(\"Epoch: {}, iteration: {}\\n\".format(epoch, b))\n",
    "                        log.write(\"Autoencoder Loss: {}\\n\".format(a_loss))\n",
    "                        log.write(\"Discriminator Gauss Loss: {}\".format(d_g_loss))\n",
    "                        log.write(\"Discriminator Categorical Loss: {}\".format(d_c_loss))\n",
    "                        log.write(\"Generator Loss: {}\\n\".format(g_loss))\n",
    "                        log.write(\"Supervised Loss: {}\".format(s_loss))\n",
    "                step += 1\n",
    "                \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                # Calculate the accuracy on validation set\n",
    "                acc = 0\n",
    "                # num_batches = int(mnist.validation.num_examples/batch_size)\n",
    "                num_batches = int(validation_size/batch_size)\n",
    "                for j in range(num_batches):\n",
    "                    # Classify unseen validation data instead of test data or train data\n",
    "                    # batch_x_l, batch_y_l = mnist.validation.next_batch(batch_size=batch_size)\n",
    "                    batch_x_l, batch_y_l = data_helper(validation, sess)\n",
    "                    encoder_acc = sess.run(accuracy, feed_dict={x_input_l: batch_x_l, y_input: batch_y_l})\n",
    "                    acc += encoder_acc\n",
    "                avg_acc = acc/num_batches\n",
    "                accuracies.append(avg_acc)\n",
    "                print(\"Encoder Classification Accuracy: {}\".format(avg_acc))\n",
    "                with open(log_path + '/log.txt', 'a') as log:\n",
    "                    log.write(\"Encoder Classification Accuracy: {}\".format(avg_acc))\n",
    "\n",
    "                # Eearly stopping\n",
    "                if avg_acc < best_acc:\n",
    "                    save_sess = sess\n",
    "                    best_acc = avg_acc\n",
    "                    last_improvement = 0\n",
    "                else:\n",
    "                    last_improvement += 1\n",
    "                if last_improvement > require_improvement:\n",
    "                    stop = True\n",
    "                    sess = save_sess\n",
    "    \n",
    "            saver.save(sess, save_path=saved_model_path, global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab14f9-86d3-46de-bd11-bf572b6887fb",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4bc72e2-95e6-4541-960a-35e42da553ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Results/all/2021-06-30 10:04:49.877833_100_0.001_128_1000_0.9_Semi_Supervised/Saved_models/-318960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Results/all/2021-06-30 10:04:49.877833_100_0.001_128_1000_0.9_Semi_Supervised/Saved_models/-318960\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# test_paths = glob.glob(data_path + 'test')\n",
    "with tf.Session() as sess:\n",
    "    attack = 'DoS'\n",
    "    data_path = './Data/{}/'.format(attack)\n",
    "    results_path = './Results/{}/'.format(attack)\n",
    "    data_info = json.load(open(data_path + 'datainfo.txt'))\n",
    "    \n",
    "    # Get the latest results folder\n",
    "    all_results = os.listdir(results_path)\n",
    "    all_results.sort()\n",
    "    \n",
    "    #saver.restore(sess, save_path=tf.train.latest_checkpoint(results_path + '/' +\n",
    "    #                                                         all_results[1] + '/Saved_models/'))\n",
    "    saver.restore(sess, save_path=tf.train.latest_checkpoint('./Results/all/2021-06-30 10:04:49.877833_100_0.001_128_1000_0.9_Semi_Supervised/Saved_models'))\n",
    "    \n",
    "    # produce_test_result()\n",
    "    test_size = data_info['test']\n",
    "    #test = data_from_tfrecord(test_paths, test_batch_size, 1)\n",
    "    test = data_from_tfrecord(data_path + 'test', batch_size, 1)\n",
    "\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        output_label, _ = encoder(x_input_l, reuse=True, supervised=True)\n",
    "\n",
    "    pred_label = tf.argmax(output_label, 1)\n",
    "    num_batches = int(test_size / batch_size)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for _ in range(num_batches):\n",
    "        x_test, y_test = data_helper(test, sess)\n",
    "        batch_pred = sess.run(pred_label, feed_dict={x_input_l: x_test})\n",
    "        batch_label = np.argmax(y_test, axis=1)\n",
    "\n",
    "        y_pred += batch_pred.tolist()\n",
    "        y_true += batch_label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bab99fd0-fe54-48f3-a9c4-79b80b274365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb75c63f-e9d3-4f56-bb74-1673d7b3eab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negative rate:  0.006910531650234958\n",
      "Error rate:  0.008954683680595972\n",
      "Precision:  0.9770439690938154\n",
      "Recall:  0.9930894683497651\n",
      "F1 score:  0.9850013784657752\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "fnr = fn/(tp + fn)\n",
    "err = (fn + fp) / (tp + tn + fp + fn)\n",
    "precision = tp/(tp + fp)\n",
    "recall = 1 - fnr\n",
    "f1score = (2 * precision * recall) / (precision + recall)\n",
    "print('False negative rate: ', fnr)\n",
    "print('Error rate: ', err)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1 score: ', f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e408f2-05be-448b-8969-775169bbb595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow1.x] *",
   "language": "python",
   "name": "conda-env-Tensorflow1.x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
