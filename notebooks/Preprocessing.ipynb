{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4862dc0-9732-4a4b-bc24-81185d3ed8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import vaex\n",
    "import numpy as np\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eb7a90-50e4-4c49-a3cc-f946aa13504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_flag(sample):\n",
    "    if not isinstance(sample['Flag'], str):\n",
    "        col = 'Data' + str(sample['DLC'])\n",
    "        sample['Flag'] = sample[col]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c5069d-5c41-4f90-915c-2229bb0605f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_canid_bits(cid):\n",
    "    try:\n",
    "        s = bin(int(str(cid), 16))[2:].zfill(29)\n",
    "        bits = list(map(int, list(s)))\n",
    "        return bits\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a1f406-b7b9-40ce-b4d5-c45a28c72d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/Car-Hacking/Fuzzy_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Read by dask first\n",
    "attributes = ['Timestamp', 'canID', 'DLC', \n",
    "                           'Data0', 'Data1', 'Data2', \n",
    "                           'Data3', 'Data4', 'Data5', \n",
    "                           'Data6', 'Data7', 'Flag']\n",
    "folder = './Data/Car-Hacking/'\n",
    "attack_types = ['DoS', 'Fuzzy', 'gear', 'RPM']\n",
    "attack = attack_types[1]\n",
    "file_name = '{}{}_dataset.csv'.format(folder, attack)\n",
    "print(file_name)\n",
    "# df = pd.read_csv(file_name, header=None, names=attributes)\n",
    "# for f in files[1]:\n",
    "#     print('Reading file: ', f)\n",
    "#     df = df.append(pd.read_csv(f, header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a0ac6d-4b12-4c56-8e7d-9a3241dec98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_name):\n",
    "    df = dd.read_csv(file_name, header=None, names=attributes)\n",
    "    print('Reading from {}: DONE'.format(file_name))\n",
    "    print('Dask processing: -------------')\n",
    "    df = df.apply(fill_flag, axis=1)\n",
    "    pd_df = df.compute()\n",
    "    pd_df = pd_df[['Timestamp', 'canID', 'Flag']].sort_values('Timestamp',  ascending=True)\n",
    "    pd_df['canBits'] = pd_df.canID.apply(convert_canid_bits)\n",
    "    pd_df['Flag'] = pd_df['Flag'].apply(lambda x: True if x == 'T' else False)\n",
    "    print('Dask processing: DONE')\n",
    "    print('Aggregate data -----------------')\n",
    "    as_strided = np.lib.stride_tricks.as_strided  \n",
    "    win = 29\n",
    "    s = 29\n",
    "    feature = as_strided(pd_df.canBits, ((len(pd_df) - win) // s + 1, win), (8*s, 8)) #Stride is counted by bytes\n",
    "    label = as_strided(pd_df.Flag, ((len(pd_df) - win) // s + 1, win), (1*s, 1))\n",
    "    df = pd.DataFrame({\n",
    "        'features': pd.Series(feature.tolist()),\n",
    "        'label': pd.Series(label.tolist())\n",
    "    }, index= range(len(feature)))\n",
    "\n",
    "    df['label'] = df['label'].apply(lambda x: 1 if any(x) else 0)\n",
    "    print('Preprocessing: DONE')\n",
    "    print('#Normal: ', df[df['label'] == 0].shape[0])\n",
    "    print('#Attack: ', df[df['label'] == 1].shape[0])\n",
    "    return df[['features', 'label']].reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4993ad-c768-4d6e-b44e-24bc32160816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from ./Data/Car-Hacking/Fuzzy_dataset.csv: DONE\n",
      "Dask processing: -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/dask/dataframe/core.py:4503: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'object', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'object', 'Data7': 'object', 'Flag': 'object'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "#Normal:  87888\n",
      "#Attack:  44486\n"
     ]
    }
   ],
   "source": [
    "df = preprocess(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01fb0407-be4e-46e3-b1d5-fddf305b910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from ./Data/Car-Hacking/Fuzzy_dataset.csv: DONE\n",
      "Dask processing: -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/dask/dataframe/core.py:4503: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'object', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'object', 'Data7': 'object', 'Flag': 'object'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>canID</th>\n",
       "      <th>Flag</th>\n",
       "      <th>canBits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.478196e+09</td>\n",
       "      <td>0545</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.478196e+09</td>\n",
       "      <td>02b0</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.478196e+09</td>\n",
       "      <td>0002</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.478196e+09</td>\n",
       "      <td>0153</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.478196e+09</td>\n",
       "      <td>0130</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125863</th>\n",
       "      <td>1.478201e+09</td>\n",
       "      <td>018f</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125864</th>\n",
       "      <td>1.478201e+09</td>\n",
       "      <td>0260</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125865</th>\n",
       "      <td>1.478201e+09</td>\n",
       "      <td>02a0</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125866</th>\n",
       "      <td>1.478201e+09</td>\n",
       "      <td>0329</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125867</th>\n",
       "      <td>1.478201e+09</td>\n",
       "      <td>0545</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3838860 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Timestamp canID   Flag  \\\n",
       "0       1.478196e+09  0545  False   \n",
       "1       1.478196e+09  02b0  False   \n",
       "2       1.478196e+09  0002  False   \n",
       "3       1.478196e+09  0153  False   \n",
       "4       1.478196e+09  0130  False   \n",
       "...              ...   ...    ...   \n",
       "125863  1.478201e+09  018f  False   \n",
       "125864  1.478201e+09  0260  False   \n",
       "125865  1.478201e+09  02a0  False   \n",
       "125866  1.478201e+09  0329  False   \n",
       "125867  1.478201e+09  0545  False   \n",
       "\n",
       "                                                  canBits  \n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                   ...  \n",
       "125863  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "125864  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "125865  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "125866  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "125867  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[3838860 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_csv(file_name, header=None, names=attributes)\n",
    "print('Reading from {}: DONE'.format(file_name))\n",
    "print('Dask processing: -------------')\n",
    "df = df.apply(fill_flag, axis=1)\n",
    "pd_df = df.compute()\n",
    "pd_df = pd_df[['Timestamp', 'canID', 'Flag']].sort_values('Timestamp',  ascending=True)\n",
    "pd_df['canBits'] = pd_df.canID.apply(convert_canid_bits)\n",
    "pd_df['Flag'] = pd_df['Flag'].apply(lambda x: True if x == 'T' else False)\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d0c06fc-0659-4d84-8ef7-87e7913a83a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1              NaN\n",
       "19        0.009996\n",
       "42        0.010007\n",
       "60        0.009996\n",
       "81        0.010007\n",
       "            ...   \n",
       "422215    1.865040\n",
       "425831    2.013793\n",
       "437309    6.547768\n",
       "446966    5.330678\n",
       "451140    3.116163\n",
       "Name: Timestamp, Length: 53477, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df[pd_df['canID'] == '02b0']['Timestamp'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56e1668-ee0c-4d7f-a212-22184877b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Normal: 3347013\n",
      "#Attack: 491847\n"
     ]
    }
   ],
   "source": [
    "print('#Normal:', pd_df[pd_df['Flag'] == False].shape[0])\n",
    "print('#Attack:', pd_df[pd_df['Flag'] == True].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "013b82fc-ebd7-4e94-b127-ce2c026ba88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_strided = np.lib.stride_tricks.as_strided  \n",
    "win = 29\n",
    "s = 29\n",
    "feature = as_strided(pd_df.canBits, ((len(pd_df) - win) // s + 1, win), (8*s, 8))\n",
    "label = as_strided(pd_df.Flag, ((len(pd_df) - win) // s + 1, win), (1*s, 1))\n",
    "df = pd.DataFrame({\n",
    "    'features': pd.Series(feature.tolist()),\n",
    "    'label': pd.Series(label.tolist())\n",
    "}, index= range(len(feature)))\n",
    "\n",
    "df['label'] = df['label'].apply(lambda x: 1 if any(x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e54c354-3ee2-41bc-b837-d0caeff3853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Normal:  87888\n",
      "#Attack:  44486\n"
     ]
    }
   ],
   "source": [
    "print('#Normal: ', df[df['label'] == 0].shape[0])\n",
    "print('#Attack: ', df[df['label'] == 1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "694dd59d-b70d-49b7-ae95-2ccc1dad8e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(df['label'] == 0)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fe5b68c-e369-4ba2-8f1a-9b14407f1b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack: Fuzzy ===============\n",
      "Reading from ./Data/Car-Hacking/Fuzzy_dataset.csv: DONE\n",
      "Dask processing: -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/dask/dataframe/core.py:4503: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'object', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'object', 'Data7': 'object', 'Flag': 'object'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1290001it [07:33, 2847.35it/s]\n",
      "2548831it [14:31, 2925.12it/s]\n",
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/dask/dataframe/core.py:4503: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'object', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'object', 'Data7': 'object', 'Flag': 'object'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack: gear ===============\n",
      "Reading from ./Data/Car-Hacking/gear_dataset.csv: DONE\n",
      "Dask processing: -------------\n",
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1893041it [10:39, 2957.95it/s]\n",
      "2550073it [14:00, 3035.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack: RPM ===============\n",
      "Reading from ./Data/Car-Hacking/RPM_dataset.csv: DONE\n",
      "Dask processing: -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/dask/dataframe/core.py:4503: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'object', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'object', 'Data7': 'object', 'Flag': 'object'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2069648it [11:10, 3086.33it/s]\n",
      "2552026it [13:58, 3042.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for attack in attack_types[1:]:\n",
    "    print('Attack: {} ==============='.format(attack))\n",
    "    file_name = '{}{}_dataset.csv'.format(folder, attack)\n",
    "    df = preprocess(file_name)\n",
    "    write_tfrecord(df[df['label'] == 1], './Data/TFRecord/{}'.format(attack))\n",
    "    write_tfrecord(df[df['label'] == 0], './Data/TFRecord/Normal_{}'.format(attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bed741d-6ad9-4ef5-9512-3983b4e313d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(df):\n",
    "    print('Create train - test - val: ')\n",
    "    train, test = train_test_split(df, test_size=0.3, shuffle=True)\n",
    "    train, val = train_test_split(train, test_size=0.2, shuffle=True)\n",
    "    train_ul, train_l = train_test_split(train, test_size=0.1, shuffle=True)\n",
    "    train_ul = train_ul.reset_index().drop(['index'], axis=1)\n",
    "    train_l = train_l.reset_index().drop(['index'], axis=1)\n",
    "    test = test.reset_index().drop(['index'], axis=1)\n",
    "    val = val.reset_index().drop(['index'], axis=1)\n",
    "    \n",
    "    data_info = {\n",
    "        \"train_unlabel\": train_ul.shape[0],\n",
    "        \"train_label\": train_l.shape[0],\n",
    "        \"validation\": val.shape[0],\n",
    "        \"test\": test.shape[0]\n",
    "    }\n",
    "    \n",
    "    return data_info, train_ul, train_l, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3279e1e7-038a-48c7-a643-ef50b9b93b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord(data, filename):\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(filename)\n",
    "    for _, row in tqdm(data.iterrows()):\n",
    "        tfrecord_writer.write(serialize_example(row['features'], row['label']))\n",
    "    tfrecord_writer.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e5ecbc-5621-4e05-8cb9-49a1fd9b8315",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 9 µs, total: 9 µs\n",
      "Wall time: 16.7 µs\n",
      "./Data/Car-Hacking/Fuzzy_dataset.csv---------------------------\n",
      "Reading from ./Data/Car-Hacking/Fuzzy_dataset.csv: DONE\n",
      "Dask processing: -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/dask/dataframe/core.py:4503: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'object', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'object', 'Data7': 'object', 'Flag': 'object'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "Create train - test - val: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  ./Data/Fuzzy/\n",
      "Writing train_unlabel.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1934770it [10:54, 2954.10it/s]\n",
      "570it [00:00, 2896.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_label.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "214975it [01:11, 2997.53it/s]\n",
      "64it [00:00, 638.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1151650it [06:20, 3026.12it/s]\n",
      "184it [00:00, 1837.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing val.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "537437it [03:06, 2876.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data info\n",
      "==========================================\n",
      "./Data/Car-Hacking/gear_dataset.csv---------------------------\n",
      "Reading from ./Data/Car-Hacking/gear_dataset.csv: DONE\n",
      "Dask processing: -------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/dask/dataframe/core.py:4503: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'object', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'object', 'Data7': 'object', 'Flag': 'object'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "Create train - test - val: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  ./Data/gear/\n",
      "Writing train_unlabel.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2239328it [12:46, 2921.27it/s]\n",
      "238it [00:00, 2379.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_label.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "248815it [01:29, 2769.91it/s]\n",
      "315it [00:00, 1836.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1332935it [07:37, 2912.73it/s]\n",
      "145it [00:00, 1448.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing val.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "622036it [03:25, 3025.76it/s]\n",
      "/home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/dask/dataframe/core.py:4503: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'Timestamp': 'float64', 'canID': 'object', 'DLC': 'int64', 'Data0': 'object', 'Data1': 'object', 'Data2': 'object', 'Data3': 'object', 'Data4': 'object', 'Data5': 'object', 'Data6': 'object', 'Data7': 'object', 'Flag': 'object'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data info\n",
      "==========================================\n",
      "./Data/Car-Hacking/RPM_dataset.csv---------------------------\n",
      "Reading from ./Data/Car-Hacking/RPM_dataset.csv: DONE\n",
      "Dask processing: -------------\n",
      "Dask processing: DONE\n",
      "Aggregate data -----------------\n",
      "Preprocessing: DONE\n",
      "Create train - test - val: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  ./Data/RPM/\n",
      "Writing train_unlabel.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2329322it [12:51, 3019.52it/s]\n",
      "252it [00:00, 2514.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_label.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "258814it [01:23, 3106.57it/s]\n",
      "1it [00:00,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386503it [07:30, 3074.51it/s]\n",
      "463it [00:00, 2450.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing val.......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "647035it [03:27, 3111.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data info\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for attack in attack_types[1:]:\n",
    "    file_name = '{}{}_dataset.csv'.format(folder, attack)\n",
    "    print(file_name + '---------------------------')\n",
    "    df = preprocess(file_name)\n",
    "    data_info, train_ul, train_l, val, test = create_train_test(df)\n",
    "    save_path = './Data/{}/'.format(attack)\n",
    "    print('Path: ', save_path)\n",
    "    print('Writing train_unlabel.......................')\n",
    "    write_tfrecord(train_ul, save_path + \"train_unlabel\")\n",
    "    print('Writing train_label.......................')\n",
    "    write_tfrecord(train_l, save_path + \"train_label\")\n",
    "    print('Writing test.......................')\n",
    "    write_tfrecord(test, save_path + \"test\")\n",
    "    print('Writing val.......................')\n",
    "    write_tfrecord(val, save_path + \"val\")\n",
    "    print('Writing data info')\n",
    "    json.dump(data_info, open(save_path + 'datainfo.txt', 'w'))\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4246e0f8-fdde-47bc-b810-a09f367c7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_path + 'datainfo.txt', 'w') as f:\n",
    "#     f.write('Train Unlabel: {}\\n'.format(train_ul.shape[0]))\n",
    "#     f.write('Train Label: {}\\n'.format(train_l.shape[0]))\n",
    "#     f.write('Test: {}\\n'.format(test.shape[0]))\n",
    "#     f.write('Validation: {}\\n'.format(val.shape[0]))\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ec23c-cc04-41c8-b14f-7a9dc3fac8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data = tf.data.TFRecordDataset('data')\n",
    "# feature_description = {\n",
    "#     'input_features': tf.io.FixedLenFeature([29*29], tf.int64),\n",
    "#     'label': tf.io.FixedLenFeature([1], tf.int64)\n",
    "# }\n",
    "\n",
    "# def _parse_image_function(example_proto):\n",
    "#   # Parse the input tf.train.Example proto using the dictionary above.\n",
    "#   return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "# parsed_image_dataset = raw_data.map(_parse_image_function)\n",
    "# parsed_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa34d96-9a47-4bc5-a210-6cf714cf9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(x, y):\n",
    "    \"\"\"converts x, y to tf.train.Example and serialize\"\"\"\n",
    "    #Need to pay attention to whether it needs to be converted to numpy() form\n",
    "    input_features = tf.train.Int64List(value = np.array(x).flatten())\n",
    "    label = tf.train.Int64List(value = np.array([y]))\n",
    "    features = tf.train.Features(\n",
    "        feature = {\n",
    "            \"input_features\": tf.train.Feature(int64_list = input_features),\n",
    "            \"label\" : tf.train.Feature(int64_list = label)\n",
    "        }\n",
    "    )\n",
    "    example = tf.train.Example(features = features)\n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc42c97-20d8-43d9-8c88-3198386dc8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(example):\n",
    "    input_dim = 841\n",
    "    feature_description = {\n",
    "    'input_features': tf.io.FixedLenFeature([input_dim], tf.int64),\n",
    "    'label': tf.io.FixedLenFeature([1], tf.int64)\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836e2072-15cc-461d-90ed-0fedd36faa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_tfrecord(tf_filepath, batch_size, repeat_time):\n",
    "    data = tf.data.TFRecordDataset(tf_filepath)\n",
    "    data = data.map(read_tfrecord)\n",
    "    data = data.shuffle(2)\n",
    "    data = data.repeat(repeat_time + 1)\n",
    "    data = data.batch(batch_size)\n",
    "    # print(tf.data.experimental.cardinality(data))\n",
    "    iterator = data.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aae3872-652c-4df1-b461-e3c82f85b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_helper(data_tf, sess):\n",
    "    n_labels = 2\n",
    "    data = sess.run(data_tf)\n",
    "    x, y = data['input_features'], data['label']\n",
    "    size = x.shape[0]\n",
    "    y_one_hot = np.eye(n_labels)[y].reshape([size, n_labels])\n",
    "    return x, y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece39933-4b4d-4cd3-a489-a7de0a069abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(file_path):\n",
    "    dataset = data_from_tfrecord(file_path, 1000, 0)\n",
    "    # print(tf.data.experimental.cardinality(dataset).numpy())\n",
    "    init = tf.global_variables_initializer()\n",
    "    size = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        while True:\n",
    "            try:\n",
    "                x_l, y_l = data_helper(dataset, sess)\n",
    "                size += x_l.shape[0]\n",
    "            except Exception as e:\n",
    "                print(type(e).__name__)\n",
    "                break\n",
    "                \n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f13df7-17b3-47a0-ac47-fd23cb277aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'./Data/TFRecord/RPM': 2069648,\n",
       " './Data/TFRecord/DoS': 1085895,\n",
       " './Data/TFRecord/Normal_Fuzzy': 2548831,\n",
       " './Data/TFRecord/Normal_DoS': 2579848,\n",
       " './Data/TFRecord/gear': 1893041,\n",
       " './Data/TFRecord/Normal_gear': 2550073,\n",
       " './Data/TFRecord/Fuzzy': 1290001,\n",
       " './Data/TFRecord/Normal_RPM': 2552026}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_info = json.load(open('./Data/TFRecord/datainfo.txt'))\n",
    "data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3edb6c-87f8-4dde-abbc-e6e1b643488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord(data, filename):\n",
    "    print('Writing {}================= '.format(filename))\n",
    "    iterator = data.make_one_shot_iterator().get_next()\n",
    "    init = tf.global_variables_initializer()\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(filename)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        while True:\n",
    "            try:\n",
    "                batch_data = sess.run(iterator)\n",
    "                for x, y in zip(batch_data['input_features'], batch_data['label']):\n",
    "                    tfrecord_writer.write(serialize_example(x, y))\n",
    "            except:\n",
    "                break\n",
    "            \n",
    "    tfrecord_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85459de4-8aec-4020-ab44-50f3a2a85a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(source_path, dest_path, DATASET_SIZE,\\\n",
    "                     train_size = 500 * 1000, train_label_size = 100 * 1000):\n",
    "    # dataset = data_from_tfrecord('./Data/TFRecord/DoS', 1000, 0)\n",
    "    #DATASET_SIZE = data_info['./Data/TFRecord/DoS']\n",
    "    #train_size = 500 * 1000\n",
    "    #train_label_size = 100 * 1000\n",
    "    val_size = int((DATASET_SIZE - train_size) * 0.2)\n",
    "    test_size = DATASET_SIZE - train_size - val_size\n",
    "    print(train_size, val_size, test_size)\n",
    "    dataset = tf.data.TFRecordDataset(source_path)\n",
    "    dataset = dataset.shuffle(1000000)\n",
    "    dataset = dataset.map(read_tfrecord)\n",
    "    train = dataset.take(train_size)\n",
    "    train_label = train.take(train_label_size)\n",
    "    train_unlabel = train.skip(train_label_size)\n",
    "    val = dataset.skip(train_size)\n",
    "    test = val.skip(val_size)\n",
    "    val = val.take(val_size)\n",
    "    batch_size = 10000\n",
    "    train_label = train_label.batch(batch_size)\n",
    "    train_unlabel = train_unlabel.batch(batch_size)\n",
    "    test = test.batch(batch_size)\n",
    "    val = val.batch(batch_size)\n",
    "\n",
    "    train_test_info = {\n",
    "        \"train_unlabel\": train_size - train_label_size,\n",
    "        \"train_label\": train_label_size,\n",
    "        \"validation\": val_size,\n",
    "        \"test\": test_size\n",
    "    }\n",
    "    json.dump(train_test_info, open(dest_path + 'datainfo.txt', 'w'))\n",
    "    write_tfrecord(train_label, dest_path + 'train_label')\n",
    "    write_tfrecord(train_unlabel, dest_path + 'train_unlabel')\n",
    "    write_tfrecord(test, dest_path + 'test')\n",
    "    write_tfrecord(val, dest_path + 'val')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a39064c-7791-4f1d-ac55-a550ab23e16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack: DoS ==============\n",
      "500000 117179 468716\n",
      "Writing ./Data/DoS/train_label================= \n",
      "WARNING:tensorflow:From <ipython-input-8-7da82c56afea>:3: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "Writing ./Data/DoS/train_unlabel================= \n",
      "Writing ./Data/DoS/test================= \n",
      "Writing ./Data/DoS/val================= \n",
      "Attack: Fuzzy ==============\n",
      "500000 158000 632001\n",
      "Writing ./Data/Fuzzy/train_label================= \n",
      "Writing ./Data/Fuzzy/train_unlabel================= \n",
      "Writing ./Data/Fuzzy/test================= \n",
      "Writing ./Data/Fuzzy/val================= \n",
      "Attack: gear ==============\n",
      "500000 278608 1114433\n",
      "Writing ./Data/gear/train_label================= \n",
      "Writing ./Data/gear/train_unlabel================= \n",
      "Writing ./Data/gear/test================= \n",
      "Writing ./Data/gear/val================= \n",
      "Attack: RPM ==============\n",
      "500000 313929 1255719\n",
      "Writing ./Data/RPM/train_label================= \n",
      "Writing ./Data/RPM/train_unlabel================= \n",
      "Writing ./Data/RPM/test================= \n",
      "Writing ./Data/RPM/val================= \n",
      "CPU times: user 24min 25s, sys: 1min 48s, total: 26min 14s\n",
      "Wall time: 21min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_info = json.load(open('./Data/TFRecord/datainfo.txt'))\n",
    "attack_types = ['DoS', 'Fuzzy', 'gear', 'RPM']\n",
    "for attack in attack_types:\n",
    "    print(\"Attack: {} ==============\".format(attack))\n",
    "    source = './Data/TFRecord/{}'.format(attack)\n",
    "    dest = './Data/{}/'.format(attack)\n",
    "    train_test_split(source, dest, data_info[source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f71e0c64-f4c7-4e97-b44e-60da7f82ea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000000 846155 3384623\n",
      "Writing ./Data/Normal/train_label================= \n",
      "WARNING:tensorflow:From <ipython-input-9-7da82c56afea>:3: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "Writing ./Data/Normal/train_unlabel================= \n",
      "Writing ./Data/Normal/test================= \n",
      "Writing ./Data/Normal/val================= \n",
      "CPU times: user 42min 37s, sys: 3min 41s, total: 46min 19s\n",
      "Wall time: 36min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "normal_size = 0\n",
    "data_info = json.load(open('./Data/TFRecord/datainfo.txt'))\n",
    "attack_types = ['DoS', 'Fuzzy', 'gear', 'RPM']\n",
    "for attack in attack_types:\n",
    "    normal_size += data_info['./Data/TFRecord/Normal_{}'.format(attack)]\n",
    "sources = ['./Data/TFRecord/Normal_{}'.format(a) for a in attack_types]\n",
    "dest = './Data/Normal/'\n",
    "train_test_split(sources, dest, normal_size, train_size=500*1000*4*3, train_label_size=100*1000*4*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ed64f18-7885-48ba-b65a-bb02f4ad96cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000 117179 468716\n",
      "Writing ./Data/DoS/train_label================= \n",
      "Writing ./Data/DoS/train_unlabel================= \n",
      "Writing ./Data/DoS/test================= \n",
      "Writing ./Data/DoS/val================= \n"
     ]
    }
   ],
   "source": [
    "# dataset = data_from_tfrecord('./Data/TFRecord/DoS', 1000, 0)\n",
    "DATASET_SIZE = data_info['./Data/TFRecord/DoS']\n",
    "train_size = 500 * 1000\n",
    "train_label_size = 100 * 1000\n",
    "val_size = int((DATASET_SIZE - train_size) * 0.2)\n",
    "test_size = DATASET_SIZE - train_size - val_size\n",
    "print(train_size, val_size, test_size)\n",
    "dataset = tf.data.TFRecordDataset('./Data/TFRecord/DoS')\n",
    "dataset = dataset.map(read_tfrecord)\n",
    "dataset = dataset.shuffle(2)\n",
    "train = dataset.take(train_size)\n",
    "train_label = train.take(train_label_size)\n",
    "train_unlabel = train.skip(train_label_size)\n",
    "val = dataset.skip(train_size)\n",
    "test = val.skip(val_size)\n",
    "val = val.take(val_size)\n",
    "batch_size = 10000\n",
    "train = train.batch(batch_size)\n",
    "test = test.batch(batch_size)\n",
    "val = val.batch(batch_size)\n",
    "\n",
    "write_tfrecord(train_label, './Data/DoS/train_label')\n",
    "write_tfrecord(train_unlabel, './Data/DoS/train_unlabel')\n",
    "write_tfrecord(test, './Data/DoS/test')\n",
    "write_tfrecord(val, './Data/DoS/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f17872-ba42-41ba-a3b8-76a9ae39e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_info = {\n",
    "        \"train_unlabel\": train_size - train_label_size,\n",
    "        \"train_label\": train_label_size,\n",
    "        \"validation\": val_size,\n",
    "        \"test\": test_size\n",
    "}\n",
    "json.dump(data_info, open(save_path + 'datainfo.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e193f4d2-2d33-4b82-a205-7d944e3d9f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-c8e92c9777a1>:1: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "iterator = train.make_one_shot_iterator().get_next()\n",
    "init = tf.global_variables_initializer()\n",
    "size = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    data = sess.run(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe4a0a1e-6e96-42ec-8720-ab6e00b599b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "close() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e93efbc22ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtfrecord_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialize_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtfrecord_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: close() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "tfrecord_writer = tf.io.TFRecordWriter('./Data/DoS/train')\n",
    "for x, y in tqdm.tqdm(zip(data['input_features'], data['label'])):\n",
    "    tfrecord_writer.write(serialize_example(x, y))\n",
    "tfrecord_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a371a9-cb5c-4988-95e0-8442f8e07141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Data/RPM/train_unlabel',\n",
       " './Data/DoS/train_unlabel',\n",
       " './Data/Normal/train_unlabel',\n",
       " './Data/gear/train_unlabel',\n",
       " './Data/Fuzzy/train_unlabel']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './Data/*/'\n",
    "glob.glob(data_path + 'train_unlabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f7c37e0-5f38-4177-9db2-b89eab2e86cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.data.Dataset.range(1, 4)\n",
    "b = tf.data.Dataset.range(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "478e132b-019d-4c3f-8118-feb5393aaf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.concatenate(b)\n",
    "a = a.shuffle(100)\n",
    "a = a.batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "863ed158-57fd-4315-bfd4-66cdbaaf9fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = a.make_one_shot_iterator().get_next()\n",
    "init = tf.global_variables_initializer()\n",
    "size = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    data = sess.run(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31a852d8-fff0-4a10-a537-14b8fd50289e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 2, 8, 9, 7, 1, 3, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dbfccf5-39c9-483e-8eac-2b89c9075a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "966e5b30-9d72-42fa-9191-16220e253b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000\n",
    "tf_filepath_unknown = ['./Data/DoS/train_unlabel']\n",
    "unknown_tfdata = tf.data.Dataset.from_tensor_slices(tf_filepath_unknown)\n",
    "unknown_tfdata = unknown_tfdata.interleave(lambda x: tf.data.TFRecordDataset(x),cycle_length=len(tf_filepath_unknown), block_length=10000)\n",
    "unknown_tfdata = unknown_tfdata.shuffle(100000, reshuffle_each_iteration=True)\n",
    "unknown_tfdata = unknown_tfdata.map(read_tfrecord, num_parallel_calls=64)\n",
    "unknown_tfdata = unknown_tfdata.batch(size)\n",
    "iterator = unknown_tfdata.make_one_shot_iterator().get_next()\n",
    "init = tf.global_variables_initializer()\n",
    "size = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    unknown_data = sess.run(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0038156-af1c-4ec5-ac55-da73ac0f88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_tfdata = tf.data.Dataset.from_tensor_slices(unknown_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fed5045b-f775-4494-a842-aa0b76206383",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Two datasets to concatenate have different types <dtype: 'string'> and {'input_features': tf.int64, 'label': tf.int64}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-14c067187455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcycle_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknown_tfdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshuffle_each_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_tfrecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1853\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1855\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    775\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \"\"\"\n\u001b[0;32m--> 777\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenateDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, dataset_to_concatenate)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       raise TypeError(\n\u001b[1;32m   2854\u001b[0m           \u001b[0;34m\"Two datasets to concatenate have different types %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m           (output_types, get_legacy_output_types(dataset_to_concatenate)))\n\u001b[0m\u001b[1;32m   2856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m     \u001b[0moutput_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_legacy_output_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Two datasets to concatenate have different types <dtype: 'string'> and {'input_features': tf.int64, 'label': tf.int64}"
     ]
    }
   ],
   "source": [
    "tf_filepath = ['./Data/Fuzzy/train_unlabel']\n",
    "repeat_time = 1\n",
    "batch_size = 100\n",
    "data = tf.data.Dataset.from_tensor_slices(tf_filepath)\n",
    "data = data.interleave(lambda x: tf.data.TFRecordDataset(x),cycle_length=len(tf_filepath), block_length=10000)\n",
    "data = data.concatenate(unknown_tfdata)\n",
    "data = data.shuffle(100000, reshuffle_each_iteration=True)\n",
    "data = data.map(read_tfrecord, num_parallel_calls=64)\n",
    "data = data.repeat(repeat_time + 1)\n",
    "data = data.batch(batch_size)\n",
    "data = data.prefetch(1)\n",
    "iterator = data.make_one_shot_iterator().get_next()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e39bd8-2305-4f9b-a810-f55ea28f6663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow1.x] *",
   "language": "python",
   "name": "conda-env-Tensorflow1.x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
