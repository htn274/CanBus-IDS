{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9d04cc-669e-46d2-8309-b8670ebd5abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fa4cbf-5cb8-434a-a7bb-0f3cdb0c34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AAE import AAE\n",
    "from CAAE import CAAE\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe53a902-b17f-4e23-996b-f1a0229bbca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 29 * 29\n",
    "n_l1 = 1000\n",
    "n_l2 = 1000\n",
    "z_dim = 10\n",
    "batch_size = 100\n",
    "n_epochs = 500\n",
    "# learning_rate = 0.001\n",
    "supervised_lr = 0.0001\n",
    "reconstruction_lr = 0.0001\n",
    "regularization_lr = 0.0001\n",
    "beta1 = 0.9\n",
    "n_labels = 2\n",
    "\n",
    "# model = AAE(input_dim, n_l1, n_l2, z_dim, n_labels)\n",
    "model = CAAE(n_labels = n_labels, z_dim = z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811f5bd4-0ee5-4c5a-8ac9-84cac670c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for input data and the targets\n",
    "x_input = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Input')\n",
    "x_input_l = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Labeled_Input')\n",
    "y_input = tf.placeholder(dtype=tf.float32, shape=[batch_size, n_labels], name='Labels')\n",
    "x_target = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_dim], name='Target')\n",
    "real_distribution = tf.placeholder(dtype=tf.float32, shape=[batch_size, z_dim], name='Real_distribution')\n",
    "categorial_distribution = tf.placeholder(dtype=tf.float32, shape=[batch_size, n_labels],\n",
    "                                         name='Categorical_distribution')\n",
    "manual_decoder_input = tf.placeholder(dtype=tf.float32, shape=[1, z_dim + n_labels], name='Decoder_input')\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514187d6-89eb-434d-b1da-88020c7de11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thiennu/Research/IDS/cnn.py:8: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/thiennu/Research/IDS/cnn.py:33: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/thiennu/Research/IDS/cnn.py:61: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1417: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/thiennu/Research/IDS/cnn.py:42: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reconstruction Phase\n",
    "# Encoder try to predict both label and latent space of the input, which will be feed into Decoder to reconstruct the input\n",
    "# The process is optimized by autoencoder_loss which is the MSE of the decoder_output and the orginal input\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    encoder_output_label, encoder_output_latent = model.encoder(x_input)\n",
    "    decoder_input = tf.concat([encoder_output_label, encoder_output_latent], 1)\n",
    "    decoder_output = model.decoder(decoder_input)\n",
    "\n",
    "autoencoder_loss = tf.reduce_mean(tf.square(x_target - decoder_output))\n",
    "autoencoder_optimizer = tf.train.AdamOptimizer(learning_rate=reconstruction_lr, beta1=beta1).minimize(autoencoder_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5edd4aa3-e7b0-469a-92ca-d066c52eb6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thiennu/miniconda3/envs/Tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Regularization Phase\n",
    "# Train both 2 discriminator of gaussian and categorical to detect the output from encoder\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    # Discriminator for gaussian\n",
    "    d_g_real = model.discriminator_gauss(real_distribution)\n",
    "    d_g_fake = model.discriminator_gauss(encoder_output_latent, reuse=True)\n",
    "# Need to seperate dicriminator of gaussian and categorical\n",
    "with (tf.variable_scope(tf.get_variable_scope())):\n",
    "    # Discrimnator for categorical\n",
    "    d_c_real = model.discriminator_categorical(categorial_distribution)\n",
    "    d_c_fake = model.discriminator_categorical(encoder_output_label, reuse=True)\n",
    "\n",
    "# Discriminator gaussian loss \n",
    "dc_g_loss_real = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_g_real), logits=d_g_real))\n",
    "dc_g_loss_fake = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_g_fake), logits=d_g_fake))\n",
    "dc_g_loss = dc_g_loss_real + dc_g_loss_fake\n",
    "\n",
    "# Discriminator categorical loss\n",
    "dc_c_loss_real = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_c_real), logits=d_c_real))\n",
    "dc_c_loss_fake = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_c_fake), logits=d_c_fake))\n",
    "dc_c_loss = dc_c_loss_fake + dc_c_loss_real\n",
    "\n",
    "all_variables = tf.trainable_variables()\n",
    "dc_g_var = [var for var in all_variables if 'dc_g_' in var.name]\n",
    "dc_c_var = [var for var in all_variables if 'dc_c_' in var.name]\n",
    "discriminator_g_optimizer = tf.train.AdamOptimizer(learning_rate=regularization_lr/5,\n",
    "                                                       beta1=beta1).minimize(dc_g_loss, var_list=dc_g_var)\n",
    "discriminator_c_optimizer = tf.train.AdamOptimizer(learning_rate=regularization_lr/5,\n",
    "                                                       beta1=beta1).minimize(dc_c_loss, var_list=dc_c_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ddd446-63c9-46b8-9450-3ea5fab31bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator loss\n",
    "generator_g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_g_fake), logits=d_g_fake))\n",
    "generator_c_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_c_fake), logits=d_c_fake))\n",
    "generator_loss = generator_g_loss + generator_c_loss\n",
    "\n",
    "en_var = [var for var in all_variables if 'e_' in var.name]\n",
    "generator_optimizer = tf.train.AdamOptimizer(learning_rate=regularization_lr, beta1=beta1).minimize(generator_loss, var_list=en_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c7c162-8434-414c-8ed5-ab1a07fb1220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-1aaa46a110e7>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Semi-Supervised Classification Phase\n",
    "# Train encoder with a small amount of label samples\n",
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    encoder_output_label_s, encoder_output_latent_s = model.encoder(x_input_l, reuse=True, supervised=True)\n",
    "    \n",
    "# Classification accuracy of encoder\n",
    "output_label = tf.argmax(encoder_output_label_s, 1)\n",
    "correct_pred = tf.equal(output_label, tf.argmax(y_input, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "supervised_encoder_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=encoder_output_label_s))\n",
    "supervised_encoder_optimizer = tf.train.AdamOptimizer(learning_rate=supervised_lr, beta1=beta1).minimize(supervised_encoder_loss, var_list=en_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeaac64-a7c9-4386-8863-d69604a316dc",
   "metadata": {},
   "source": [
    "# Train in supervised with latent labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82c55e-ff42-4767-b177-8fd9c92dda6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# data_path = './Data/*/'\n",
    "n_labeled = 10000\n",
    "labels = ['DoS', 'Fuzzy', 'gear', 'RPM', 'Normal']\n",
    "unknown_attack = ''\n",
    "normal_label_path = ['./Data/Normal/train_label']\n",
    "attack_label_paths = ['./Data/{}/train_label'.format(l) for l in labels[:-1]]\n",
    "\n",
    "train_normal = data_from_tfrecord(normal_label_path, batch_size, 1)\n",
    "train_attack = data_from_tfrecord(attack_label_paths, batch_size, 1)\n",
    "with tf.Session() as sess:\n",
    "    results_path = './Results/all/2021-07-19 17:33:01.052325_10_0.0001_100_1000_0.9_Semi_Supervised/'\n",
    "    saver.restore(sess, save_path=tf.train.latest_checkpoint(results_path + '/Saved_models'))\n",
    "    n_batches = int(n_labeled / batch_size)\n",
    "    X = np.empty((0, 10), float)\n",
    "    y = np.empty((0), int)\n",
    "    for b in tqdm.tqdm(range(1, n_batches + 1)):\n",
    "        batch_x_l, batch_y_l = data_stream(train_normal, sess)\n",
    "        batch_label = np.argmax(batch_y_l, axis=1)\n",
    "        batch_encoded_x = sess.run(encoder_latent, feed_dict={x_input_l: batch_x_l})\n",
    "        \n",
    "        X = np.append(X, batch_encoded_x, axis=0)\n",
    "        y = np.append(y, batch_label, axis=0)\n",
    "        \n",
    "        batch_x_l, batch_y_l = data_stream(train_attack, sess)\n",
    "        batch_label = np.argmax(batch_y_l, axis=1)\n",
    "        batch_encoded_x = sess.run(encoder_latent, feed_dict={x_input_l: batch_x_l})\n",
    "        \n",
    "        X = np.append(X, batch_encoded_x, axis=0)\n",
    "        y = np.append(y, batch_label, axis=0)\n",
    "        \n",
    "    print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d1efbb7d-ff5e-4051-a29e-455569541868",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(100, input_dim=10, activation='relu'),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5685bf1e-896e-4863-946d-a0b908f81351",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d606cdb5-a341-4620-acaf-009ebd0f1461",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 0s 17us/sample - loss: 0.5717 - acc: 0.7261\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.5104 - acc: 0.7963\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 0s 11us/sample - loss: 0.4928 - acc: 0.8166\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.4803 - acc: 0.8284\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4736 - acc: 0.8361\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4644 - acc: 0.8452\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 0s 11us/sample - loss: 0.4572 - acc: 0.8534\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4532 - acc: 0.8575\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4496 - acc: 0.8623\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 0s 11us/sample - loss: 0.4469 - acc: 0.8643\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4433 - acc: 0.8676\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.4408 - acc: 0.8720\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4377 - acc: 0.8730\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 0s 11us/sample - loss: 0.4369 - acc: 0.8734\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4333 - acc: 0.8791\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4304 - acc: 0.8812\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.4289 - acc: 0.8832\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 0s 14us/sample - loss: 0.4283 - acc: 0.8835\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4247 - acc: 0.8884\n",
      "Epoch 20/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4239 - acc: 0.8895\n",
      "Epoch 21/100\n",
      "20000/20000 [==============================] - 0s 14us/sample - loss: 0.4228 - acc: 0.8896\n",
      "Epoch 22/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4215 - acc: 0.8903\n",
      "Epoch 23/100\n",
      "20000/20000 [==============================] - 0s 11us/sample - loss: 0.4203 - acc: 0.8928\n",
      "Epoch 24/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4174 - acc: 0.8961\n",
      "Epoch 25/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4160 - acc: 0.8972\n",
      "Epoch 26/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4171 - acc: 0.8959\n",
      "Epoch 27/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.4134 - acc: 0.9003\n",
      "Epoch 28/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.4126 - acc: 0.9018\n",
      "Epoch 29/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4105 - acc: 0.9028\n",
      "Epoch 30/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4110 - acc: 0.9028\n",
      "Epoch 31/100\n",
      "20000/20000 [==============================] - 0s 11us/sample - loss: 0.4106 - acc: 0.9034\n",
      "Epoch 32/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4088 - acc: 0.9061\n",
      "Epoch 33/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4094 - acc: 0.9038\n",
      "Epoch 34/100\n",
      "20000/20000 [==============================] - 0s 16us/sample - loss: 0.4064 - acc: 0.9076\n",
      "Epoch 35/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.4048 - acc: 0.9094\n",
      "Epoch 36/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.4060 - acc: 0.9067\n",
      "Epoch 37/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4045 - acc: 0.9096\n",
      "Epoch 38/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4019 - acc: 0.9128\n",
      "Epoch 39/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4033 - acc: 0.9104\n",
      "Epoch 40/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4007 - acc: 0.9130\n",
      "Epoch 41/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4017 - acc: 0.9127\n",
      "Epoch 42/100\n",
      "20000/20000 [==============================] - 0s 14us/sample - loss: 0.3994 - acc: 0.9153\n",
      "Epoch 43/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.4009 - acc: 0.9123\n",
      "Epoch 44/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3982 - acc: 0.9158\n",
      "Epoch 45/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3981 - acc: 0.9165\n",
      "Epoch 46/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3977 - acc: 0.9172\n",
      "Epoch 47/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3979 - acc: 0.9156\n",
      "Epoch 48/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3966 - acc: 0.9179\n",
      "Epoch 49/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.3963 - acc: 0.9184\n",
      "Epoch 50/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.3967 - acc: 0.9176\n",
      "Epoch 51/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3942 - acc: 0.9212\n",
      "Epoch 52/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3938 - acc: 0.9202\n",
      "Epoch 53/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3931 - acc: 0.9222\n",
      "Epoch 54/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3921 - acc: 0.9227\n",
      "Epoch 55/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3914 - acc: 0.9236\n",
      "Epoch 56/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3904 - acc: 0.9236\n",
      "Epoch 57/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3917 - acc: 0.9227\n",
      "Epoch 58/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.3914 - acc: 0.9231\n",
      "Epoch 59/100\n",
      "20000/20000 [==============================] - 0s 14us/sample - loss: 0.3903 - acc: 0.9236\n",
      "Epoch 60/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3910 - acc: 0.9225\n",
      "Epoch 61/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3912 - acc: 0.9229\n",
      "Epoch 62/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.3884 - acc: 0.9268\n",
      "Epoch 63/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.3901 - acc: 0.9240\n",
      "Epoch 64/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3876 - acc: 0.9273\n",
      "Epoch 65/100\n",
      "20000/20000 [==============================] - 0s 11us/sample - loss: 0.3868 - acc: 0.9281\n",
      "Epoch 66/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3867 - acc: 0.9275\n",
      "Epoch 67/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3860 - acc: 0.9291\n",
      "Epoch 68/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3872 - acc: 0.9268\n",
      "Epoch 69/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3882 - acc: 0.9251\n",
      "Epoch 70/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.3869 - acc: 0.9272\n",
      "Epoch 71/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.3862 - acc: 0.9283\n",
      "Epoch 72/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3841 - acc: 0.9301\n",
      "Epoch 73/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3833 - acc: 0.9312\n",
      "Epoch 74/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3821 - acc: 0.9333\n",
      "Epoch 75/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3832 - acc: 0.9314\n",
      "Epoch 76/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3819 - acc: 0.9334\n",
      "Epoch 77/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3848 - acc: 0.9295\n",
      "Epoch 78/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3827 - acc: 0.9324\n",
      "Epoch 79/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3856 - acc: 0.9279\n",
      "Epoch 80/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3812 - acc: 0.9334\n",
      "Epoch 81/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3822 - acc: 0.9322\n",
      "Epoch 82/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3806 - acc: 0.9337\n",
      "Epoch 83/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3795 - acc: 0.9352\n",
      "Epoch 84/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3801 - acc: 0.9349\n",
      "Epoch 85/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3796 - acc: 0.9351\n",
      "Epoch 86/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3801 - acc: 0.9352\n",
      "Epoch 87/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3799 - acc: 0.9350\n",
      "Epoch 88/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3789 - acc: 0.9360\n",
      "Epoch 89/100\n",
      "20000/20000 [==============================] - 0s 14us/sample - loss: 0.3779 - acc: 0.9370\n",
      "Epoch 90/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.3794 - acc: 0.9348\n",
      "Epoch 91/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3792 - acc: 0.9346\n",
      "Epoch 92/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3784 - acc: 0.9363\n",
      "Epoch 93/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.3783 - acc: 0.9360\n",
      "Epoch 94/100\n",
      "20000/20000 [==============================] - 0s 13us/sample - loss: 0.3795 - acc: 0.9354\n",
      "Epoch 95/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3782 - acc: 0.9358\n",
      "Epoch 96/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3756 - acc: 0.9389\n",
      "Epoch 97/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3775 - acc: 0.9373\n",
      "Epoch 98/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3772 - acc: 0.9372\n",
      "Epoch 99/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3754 - acc: 0.9391\n",
      "Epoch 100/100\n",
      "20000/20000 [==============================] - 0s 12us/sample - loss: 0.3766 - acc: 0.9376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa29659ab70>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da33f71-fac5-4219-8c25-32ed0bd67f8f",
   "metadata": {},
   "source": [
    "# Train K-means with latent labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e985f59-81e7-42f7-87fd-f3edbf854ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Results/unknown/DoS/CNN_2021-07-23 11:29:55.406382_10_0.0001_64_500_0.9_Semi_Supervised//Saved_models/-291500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [00:01<00:00, 327.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37300, 10) (37300,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# data_path = './Data/*/'\n",
    "n_labeled = 37372\n",
    "labels = ['Fuzzy', 'gear', 'RPM', 'Normal']\n",
    "unknown_attack = ''\n",
    "train_label_paths = ['./Data/{}/train_label'.format(l) for l in labels if l is not unknown_attack]\n",
    "train_label = data_from_tfrecord(train_label_paths, batch_size, 1)\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    encoder_label, encoder_latent = model.encoder(x_input_l, reuse=True, supervised=False)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    results_path = './Results/unknown/DoS/CNN_2021-07-23 11:29:55.406382_10_0.0001_64_500_0.9_Semi_Supervised/'\n",
    "    saver.restore(sess, save_path=tf.train.latest_checkpoint(results_path + '/Saved_models'))\n",
    "    n_batches = int(n_labeled / batch_size)\n",
    "    X = np.empty((0, 10), float)\n",
    "    y = np.empty((0), int)\n",
    "    for b in tqdm.tqdm(range(1, n_batches + 1)):\n",
    "        batch_x_l, batch_y_l = data_stream(train_label, sess)\n",
    "        batch_label = np.argmax(batch_y_l, axis=1)\n",
    "        batch_encoded_x = sess.run(encoder_latent, feed_dict={x_input_l: batch_x_l})\n",
    "        X = np.append(X, batch_encoded_x, axis=0)\n",
    "        y = np.append(y, batch_label, axis=0)\n",
    "    print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6f6398f-c943-4517-8ce6-3c6ed7a5c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc192727-3eea-402b-8ee5-251521526ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(clt, y):\n",
    "    mask = (kmeans.labels_ == clt)\n",
    "    same_cluster = y[mask]\n",
    "    try:\n",
    "        prob_attack = np.count_nonzero(same_cluster)/same_cluster.shape[0]\n",
    "    except:\n",
    "        print(clt, same_cluster.shape)\n",
    "    return int(prob_attack >= 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad997b-8c00-4450-ad3b-fd2b0d0c6ffd",
   "metadata": {},
   "source": [
    "# Train anomoly detection by latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "627c5ce3-0adc-47d4-bd68-a1fb3fe53b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b3bf220-0d64-481c-a772-2244f587058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Results/unknown/DoS/CNN_2021-07-23 11:29:55.406382_10_0.0001_64_500_0.9_Semi_Supervised///Saved_models/-291500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [00:00<00:00, 304.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24600, 12) (24600,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# data_path = './Data/*/'\n",
    "n_labeled = 24693\n",
    "# Only use labeled normal data\n",
    "train_label_paths = ['./Data/Normal/train_label']\n",
    "train_label = data_from_tfrecord(train_label_paths, batch_size, 1)\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    encoder_label, encoder_latent = model.encoder(x_input_l, reuse=True, supervised=False)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    results_path = './Results/unknown/DoS/CNN_2021-07-23 11:29:55.406382_10_0.0001_64_500_0.9_Semi_Supervised//'\n",
    "    saver.restore(sess, save_path=tf.train.latest_checkpoint(results_path + '/Saved_models'))\n",
    "    n_batches = int(n_labeled / batch_size)\n",
    "    X = np.empty((0, 12), float)\n",
    "    y = np.empty((0), int)\n",
    "    for b in tqdm.tqdm(range(1, n_batches + 1)):\n",
    "        batch_x_l, batch_y_l = data_stream(train_label, sess)\n",
    "        batch_label = np.argmax(batch_y_l, axis=1)\n",
    "        #print(batch_x_l.shape)\n",
    "        batch_encoded_l, batch_encoded_z = sess.run([encoder_label, encoder_latent], feed_dict={x_input_l: batch_x_l})\n",
    "        batch_encoded_x = np.concatenate((batch_encoded_l, batch_encoded_z), axis=1)\n",
    "        X = np.append(X, batch_encoded_x, axis=0)\n",
    "        y = np.append(y, batch_label, axis=0)\n",
    "    print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "572203ef-2298-43f4-aa1d-6ef28838a90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalOutlierFactor(novelty=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lof = LocalOutlierFactor(novelty=True)\n",
    "lof.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e241d2-7e07-438b-a3cf-0263f63b4497",
   "metadata": {},
   "source": [
    "# Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a20a322b-e54b-4620-9dc2-564efcefe110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58532"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels = ['DoS', 'Fuzzy', 'gear', 'RPM', 'Normal']\n",
    "labels = ['DoS', 'Normal']\n",
    "unknown_attack = ''\n",
    "test_size = 0\n",
    "for f in ['./Data/{}/datainfo.txt'.format(l) for l in labels]:\n",
    "    data_read = json.load(open(f))\n",
    "    test_size += data_read['test']\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c58f1d97-9810-4faf-bb65-45f5d2d36b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Data/DoS/', './Data/Normal/']\n",
      "INFO:tensorflow:Restoring parameters from ./Results/unknown/DoS/CNN_2021-07-23 11:29:55.406382_10_0.0001_64_500_0.9_Semi_Supervised//Saved_models/-291500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585/585 [00:03<00:00, 182.77it/s]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# labels = ['DoS', 'Fuzzy', 'gear', 'RPM', 'Normal']\n",
    "# labels = ['DoS', 'Normal']\n",
    "# test_paths = glob.glob(data_path + 'test')\n",
    "with tf.Session() as sess:\n",
    "    #attack = 'gear'\n",
    "    data_path = ['./Data/{}/'.format(a) for a in labels]\n",
    "    print(data_path)\n",
    "    results_path = './Results/unknown/DoS/CNN_2021-07-23 11:29:55.406382_10_0.0001_64_500_0.9_Semi_Supervised/'\n",
    "    #data_info = json.load(open(data_path + 'datainfo.txt'))\n",
    "    \n",
    "    # Get the latest results folder\n",
    "    #all_results = os.listdir(results_path)\n",
    "    #all_results.sort()\n",
    "    \n",
    "    #saver.restore(sess, save_path=tf.train.latest_checkpoint(results_path + '/' +\n",
    "    #                                                         all_results[1] + '/Saved_models/'))\n",
    "    saver.restore(sess, save_path=tf.train.latest_checkpoint(results_path + '/Saved_models'))\n",
    "    \n",
    "    test = data_from_tfrecord([p + 'test' for p in data_path], batch_size, 1)\n",
    "    \n",
    "    num_batches = int(test_size / batch_size)\n",
    "    y_true = np.empty((0), int)\n",
    "    y_pred = np.empty((0), int)\n",
    "    total_prob = np.empty((0), float)\n",
    "    total_latent = np.empty((0, z_dim + 2), float)\n",
    "    \n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        encoder_label, _ = model.encoder(x_input_l, reuse=True, supervised=False)\n",
    "    \n",
    "    for _ in tqdm.tqdm(range(num_batches)):\n",
    "        x_test, y_test = data_stream(test, sess)\n",
    "        batch_pred, batch_latent = sess.run([encoder_output_label_s, encoder_output_latent_s], feed_dict={x_input_l: x_test})\n",
    "        batch_encoded_label = sess.run(encoder_label, feed_dict={x_input_l: x_test})\n",
    "        batch_encoded = np.concatenate((batch_encoded_label, batch_latent), axis=1)\n",
    "        total_latent = np.append(total_latent, batch_encoded, axis=0)\n",
    "        batch_label = np.argmax(y_test, axis=1).reshape((batch_size))\n",
    "        prob = np.max(batch_pred, axis=1).reshape((batch_size))\n",
    "        batch_pred = np.argmax(batch_pred, axis=1).reshape((batch_size))\n",
    "        y_pred = np.append(y_pred, batch_pred, axis=0)\n",
    "        y_true = np.append(y_true, batch_label, axis=0)  \n",
    "        total_prob = np.append(total_prob, prob, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e2ba753-1284-4003-af6e-e3381801a309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5613\n",
      "8 52878\n",
      "False negative rate:  0.9998218738867118\n",
      "Error rate:  0.09608547008547008\n",
      "Precision:  0.1111111111111111\n",
      "Recall:  0.0001781261132881573\n",
      "F1 score:  0.000355682020273774\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b21b465a-c8cd-48f6-88a4-f580bfcf7405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 3.21 ms, total: 3.21 ms\n",
      "Wall time: 2.41 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "normal_indices = (y_pred == 0).nonzero()\n",
    "normal_latent = total_latent[normal_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7cdb8e4f-794f-4905-b6b3-1b05cc376ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cluster = kmeans.predict(normal_latent[:, :10])\n",
    "f = lambda x: get_class(x, y)\n",
    "vf = np.vectorize(f)\n",
    "pred_label = vf(pred_cluster)\n",
    "y_pred[normal_indices] = pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be19591a-6105-4b23-a757-00eda47b73b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5613\n",
      "8 52878\n",
      "False negative rate:  0.9998218738867118\n",
      "Error rate:  0.09608547008547008\n",
      "Precision:  0.1111111111111111\n",
      "Recall:  0.0001781261132881573\n",
      "F1 score:  0.000355682020273774\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5bac38-cf35-41c2-a9eb-53ec07256c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(normal_latent)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "y_pred[normal_indices] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dc61da53-9e28-4d22-82c9-e72f4c6308fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32540 246\n",
      "23183 29731\n",
      "False negative rate:  0.007503202586469835\n",
      "Error rate:  0.27338389731621937\n",
      "Precision:  0.5839599447265941\n",
      "Recall:  0.9924967974135301\n",
      "F1 score:  0.7352924561344044\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b1c7869-5044-465b-9a7d-d83e1740e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lof.predict(normal_latent)\n",
    "label = np.where(pred < 0, 1, 0)\n",
    "y_pred[normal_indices] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89680ff5-2e4f-4584-b707-29936f4f20b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 5591\n",
      "131 52752\n",
      "False negative rate:  0.9953711945878583\n",
      "Error rate:  0.09781196581196581\n",
      "Precision:  0.16560509554140126\n",
      "Recall:  0.004628805412141723\n",
      "F1 score:  0.009005888465535176\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82567e-fffc-424a-8c31-ed788d9a8f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow1.x] *",
   "language": "python",
   "name": "conda-env-Tensorflow1.x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
